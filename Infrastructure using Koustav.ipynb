{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INfrastructure Damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sreenu/.local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import sys,os\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/SMERP/RetrievedSMERP/level-2/SMERP_T3_level2.tsv\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1P['label']=1\n",
    "text1N=pd.read_table(\"/home/sreenu/Desktop/dataset4may/SMERP/RetrievedSMERP/level-2/NONSMERP-level2-T3.tsv\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1N['label']=0\n",
    "totalframes=[text1P,text1N]\n",
    "level2=open('smerpl2.tsv','w')\n",
    "for i in text1P['tweet1']:\n",
    "    level2.write(i+'\\t'+'1'+'\\n')\n",
    "level2.close()\n",
    "level2=open('smerpl2.tsv','a')\n",
    "for i in text1N['tweet1']:\n",
    "    level2.write(i+'\\t'+'0'+'\\n')\n",
    "level2.close()\n",
    "#trainP=text1P[:636]\n",
    "#testP=text1P[636:]\n",
    "#trainN=text1N[:636]\n",
    "#testN=text1N[636:]\n",
    "#frames=[trainP,trainN]\n",
    "#frames1=[testP,testN]\n",
    "#framesTrain=pd.concat(frames)\n",
    "#framesTest=pd.concat(frames1)\n",
    "#X_test=pd.concat(totalframes)['tweet1']\n",
    "#y_test=pd.concat(totalframes)['label']\n",
    "#X_test=pd.concat(frames1)['tweet1']\n",
    "#y_test=pd.concat(frames1)['label']\n",
    "#train=[textTrainP,trainP,textTrainN,trainN]\n",
    "#X_train=pd.concat(frames)['tweet1']\n",
    "#y_train=pd.concat(frames)['label']\n",
    "#test=[textTestP,testP,textTestN,testN]\n",
    "#X_test=pd.concat(frames1)['tweet1']\n",
    "#y_test=pd.concat(frames1)['label']\n",
    "#print len(X_train)\n",
    "#print len(X_test)\n",
    "#print len(y_train)\n",
    "#print len(y_test)\n",
    "X_train11=pd.concat(totalframes)['tweet1']\n",
    "y_train11=pd.concat(totalframes)['label']\n",
    "print len(X_train11)\n",
    "#X_train,X_test,y_train,y_test=train_test_split(X_train11,y_train11,test_size=0.25,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INfrastructure Damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/FIRE/FIRE2016-microblogs-track-data/RetrievedFIRE/FMT7tweets.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1P['label']=1\n",
    "text1N=pd.read_table(\"/home/sreenu/Desktop/dataset4may/FIRE/FIRE2016-microblogs-track-data/RetrievedFIRE/Nonrelevant.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1N['label']=0\n",
    "text1N=text1N[:241]\n",
    "totalframes=[text1P,text1N]\n",
    "trainP=text1P[:182]\n",
    "testP=text1P[182:]\n",
    "trainN=text1N[:182]\n",
    "testN=text1N[182:241]\n",
    "frames=[trainP,trainN]\n",
    "frames1=[testP,testN]\n",
    "framesTrain=pd.concat(frames)\n",
    "framesTest=pd.concat(frames1)\n",
    "#X_train=pd.concat(frames)['tweet1']\n",
    "#y_train=pd.concat(frames)['label']\n",
    "#X_test=pd.concat(frames1)['tweet1']\n",
    "#y_test=pd.concat(frames1)['label']\n",
    "\n",
    "X_test11=pd.concat(totalframes)['tweet1']\n",
    "y_test11=pd.concat(totalframes)['label']\n",
    "print len(X_test11)\n",
    "#print len(X_test)\n",
    "fire=open('fire.tsv','w')\n",
    "for i in text1P['tweet1']:\n",
    "    fire.write(i+'\\t'+'1'+'\\n')\n",
    "fire.close()\n",
    "fire=open('fire.tsv','a')\n",
    "for i in text1N['tweet1']:\n",
    "    fire.write(i+'\\t'+'0'+'\\n')\n",
    "fire.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INfrastructure Damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P1=pd.read_table(\"/home/sreenu/Desktop/TweetCrawler/CrisisNLP_volunteers_labels.DS_Store/2015_Nepal_Earthquake_en/Filtered tweets - 2015_Nepal_Earthquake_en.csv/infrastructuredamage/Infrastructureandutilities.csv\",header=None,sep=\",u\",names=['tweetid','tweet1'],encoding='utf-8')\n",
    "text1P1['label']=1\n",
    "text1P2=pd.read_table(\"/home/sreenu/Desktop/TweetCrawler/CrisisNLP_volunteers_labels.DS_Store/2015_Nepal_Earthquake_en/Filtered tweets - 2015_Nepal_Earthquake_en.csv/infrastructuredamage/Infrastructuredamage.csv\",header=None,sep=\",u\",names=['tweetid','tweet1'],encoding='utf-8')\n",
    "text1P2['label']=1\n",
    "text1P3=pd.read_table(\"/home/sreenu/Desktop/TweetCrawler/CrisisNLP_volunteers_labels.DS_Store/2015_Nepal_Earthquake_en/Filtered tweets - 2015_Nepal_Earthquake_en.csv/infrastructuredamage/Injuredordeadpeople.csv\",header=None,sep=\",u\",names=['tweetid','tweet1'],encoding='utf-8')\n",
    "text1P3['label']=1\n",
    "text1P4=pd.read_table(\"/home/sreenu/Desktop/TweetCrawler/CrisisNLP_volunteers_labels.DS_Store/2015_Nepal_Earthquake_en/Filtered tweets - 2015_Nepal_Earthquake_en.csv/infrastructuredamage/Missingtrappedorfoundpeople.csv\",header=None,sep=\",u\",names=['tweetid','tweet1'],encoding='utf-8')\n",
    "text1P4['label']=1\n",
    "#print text1P.shape\n",
    "text1N=pd.read_table(\"/home/sreenu/Desktop/TweetCrawler/CrisisNLP_volunteers_labels.DS_Store/2015_Nepal_Earthquake_en/Filtered tweets - 2015_Nepal_Earthquake_en.csv/infrastructuredamage/Otherrelevantinformation.csv\",header=None,sep=\",u\",names=['tweetid','tweet1'],encoding='utf-8')\n",
    "text1N['label']=0\n",
    "print len(text1N)\n",
    "frames=[text1P1,text1P2,text1P3,text1P4,text1N]\n",
    "result3=pd.concat(frames)\n",
    "trainP=result3[:227]\n",
    "testP=result3[227:302]\n",
    "trainN=text1N[:227]\n",
    "testN=text1N[227:]\n",
    "frames1=[trainP,trainN]\n",
    "frames2=[testP,testN]\n",
    "framesTrain=pd.concat(frames1)\n",
    "framesTest=pd.concat(frames2)\n",
    "X_train=pd.concat(frames1)['tweet1']\n",
    "y_train=pd.concat(frames1)['label']\n",
    "X_test=pd.concat(frames2)['tweet1']\n",
    "y_test=pd.concat(frames2)['label']\n",
    "#print result3.shape\n",
    "#print len(result3)\n",
    "#print text1N.shape\n",
    "#print result.label\n",
    "#text2=text1.dropna(thresh=0.8*len(text1), axis=1)\n",
    "X_test22=result3.tweet1\n",
    "y_test22=result3.label\n",
    "#print y_test22.dtype\n",
    "Nepal=open('Nepal.tsv','w')\n",
    "for i,j in zip(X_test22,y_test22):\n",
    "    Nepal.write(i+'\\t'+str(j)+'\\n')\n",
    "Nepal.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Equal size of need and availability of nepal earthquake 2015 80 p training and 20 p testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(464, 2)\n",
      "(92, 2)\n",
      "(1116,)\n",
      "(276,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sreenu/.local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/Paperswithcode/applyandcompareneuralmodels/data/nepal/Availabilitynepal.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1P['label']=1\n",
    "#print text1P.shape\n",
    "#print len(text1P)\n",
    "text1P1=pd.read_table(\"/home/sreenu/Desktop/dataset4may/Paperswithcode/applyandcompareneuralmodels/data/nepal/neednepal.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1P1['label']=2\n",
    "print text1P1.shape\n",
    "text1N=pd.read_table(\"/home/sreenu/Desktop/dataset4may/FIRE/FIRE2016-microblogs-track-data/Nonrelevantresource.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1N['label']=0\n",
    "textTrainP=text1P[:372]\n",
    "textTestP=text1P[372:464]\n",
    "textTrainP1=text1P1[:372]\n",
    "textTestP1=text1P1[372:]\n",
    "textTrainN=text1N[:372]\n",
    "textTestN=text1N[372:464]\n",
    "framesTrain=[textTrainP,textTrainP1,textTrainN]\n",
    "framesTest=[textTestP,textTestP1,textTestN]\n",
    "train=pd.concat(framesTrain)\n",
    "test=pd.concat(framesTest)\n",
    "X_train=train['tweet1']\n",
    "y_train=train['label']\n",
    "X_test=test['tweet1']\n",
    "y_test=test['label']\n",
    "print textTestP.shape\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italy Earthquake equal size need and availability 80 training and 20 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/Paperswithcode/applyandcompareneuralmodels/data/italy/availitaly.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1P['label']=1\n",
    "print text1P.shape\n",
    "#print len(text1P)\n",
    "text1P1=pd.read_table(\"/home/sreenu/Desktop/dataset4may/Paperswithcode/applyandcompareneuralmodels/data/italy/needitaly.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1P1['label']=2\n",
    "print text1P1.shape\n",
    "text1N=pd.read_table(\"/home/sreenu/Desktop/dataset4may/Paperswithcode/applyandcompareneuralmodels/data/italy/nonrelevantresource.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1N['label']=0\n",
    "textTrainP=text1P[:120]\n",
    "textTestP=text1P[120:149]\n",
    "textTrainP1=text1P1[:120]\n",
    "textTestP1=text1P1[120:]\n",
    "textTrainN=text1N[:120]\n",
    "textTestN=text1N[120:149]\n",
    "framesTrain=[textTrainP,textTrainP1,textTrainN]\n",
    "framesTest=[textTestP,textTestP1,textTestN]\n",
    "train=pd.concat(framesTrain)\n",
    "test=pd.concat(framesTest)\n",
    "X_train=train['tweet1']\n",
    "y_train=train['label']\n",
    "X_test=test['tweet1']\n",
    "y_test=test['label']\n",
    "print textTrainP.shape\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.68      0.68       293\n",
      "          1       0.68      0.68      0.68       293\n",
      "\n",
      "avg / total       0.68      0.68      0.68       586\n",
      "\n",
      "0.6808873720136519\n",
      "0.6808873720136519\n",
      "[[199  94]\n",
      " [ 93 200]]\n",
      "424 1694\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.60      0.68       209\n",
      "          1       0.68      0.83      0.75       215\n",
      "\n",
      "avg / total       0.73      0.72      0.71       424\n",
      "\n",
      "0.7169811320754716\n",
      "0.7153221319683989\n",
      "[[125  84]\n",
      " [ 36 179]]\n",
      "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0], [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0], [3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 1, 0, 1, 2, 1, 1, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 1, 0, 0, 3, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0], [5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0], [6, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 5, 1, 1, 0, 1], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0, 4, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 0, 3, 1, 1, 0, 0], [0, 1, 0, 1, 0, 1, 2, 1, 1, 0, 1], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], [4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.68      0.75        65\n",
      "          1       0.69      0.84      0.76        56\n",
      "\n",
      "avg / total       0.77      0.75      0.75       121\n",
      "\n",
      "0.7520661157024794\n",
      "0.7581043956043958\n",
      "[[44 21]\n",
      " [ 9 47]]\n",
      "160\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.70      0.58        83\n",
      "          1       0.43      0.25      0.31        77\n",
      "\n",
      "avg / total       0.47      0.48      0.45       160\n",
      "\n",
      "0.48125\n",
      "0.47277421373806916\n",
      "[[58 25]\n",
      " [58 19]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on 11-May-2015\n",
    "\n",
    "@author: Koustav\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "from textblob import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn import svm\n",
    "from sklearn.svm import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import gzip\n",
    "from happyfuntokenizing import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import *\n",
    "#from happyfuntokenizing import *\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "mycompile = lambda pat:  re.compile(pat,  re.UNICODE)\n",
    "\n",
    "PRONOUN_PATH = '../../Lexical_resources/classifier_dictionary/english_pronoun.txt'\n",
    "WHWORD_PATH = '../classifier_dictionary/english_whwords.txt'\n",
    "SLANG_PATH = '../classifier_dictionary/english_swear.txt'\n",
    "INTENSIFIER_PATH = '../classifier_dictionary/english_intensifier.txt'\n",
    "SUBJECTIVE_PATH = '../classifier_dictionary/subjclueslen1-HLTEMNLP05.tff'\n",
    "EVENT_PATH = '../classifier_dictionary/english_nonsituational_phrase.txt'\n",
    "MODAL_VERB_PATH = '../classifier_dictionary/english_modal_verb.txt'\n",
    "RELIGION_PATH = '../classifier_dictionary/communal_race.txt'\n",
    "#NONSIT_PATH = 'Common_nonsituational_word.txt'\n",
    "#OPINION_HASHTAG_PATH = '/home/krudra/twitter_code/shared/language_overlap/code_mix_pitch/devanagari/devanagari_hashtag_opinion.txt'\n",
    "#MENTION_PATH = '/home/krudra/twitter_code/shared/language_overlap/code_mix_pitch/devanagari/news_mention.txt'\n",
    "\n",
    "#TRAIN1 = '../classification_4.3/fragmented_train_data/hydb_fragment_train.txt'\n",
    "#TRAIN2 = '../classification_4.3/fragmented_train_data/utkd_fragment_train.txt'\n",
    "#TRAIN3 = '../classification_4.3/fragmented_train_data/sandy_hook_fragment_train.txt'\n",
    "#TRAIN4 = '../classification_4.3/fragmented_train_data/hagupit_fragment_train.txt'\n",
    "TRAIN1=\"../dataset4may/SMERP/RetrievedSMERP/level-1/TotalSMERP_T31.tsv\"\n",
    "TRAIN2=\"smerpl2.tsv\"\n",
    "TRAIN3=\"fire.tsv\"\n",
    "TRAIN4=\"Nepal.tsv\"\n",
    "\n",
    "'''TRAIN1 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hydb_RAW_CLASS.txt'\n",
    "TRAIN2 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/utkd_RAW_CLASS.txt'\n",
    "TRAIN3 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/sandy_hook_RAW_CLASS.txt'\n",
    "TRAIN4 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hagupit_RAW_CLASS.txt' '''\n",
    "\n",
    "'''TRAIN1 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hydb_balance_RAW.txt'\n",
    "TRAIN2 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/utkd_balance_RAW.txt'\n",
    "TRAIN3 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/sandy_hook_balance_RAW.txt'\n",
    "TRAIN4 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hagupit_balance_RAW.txt' '''\n",
    "\n",
    "\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "TAGGER_PATH = '../../Lexical_resources/ark-tweet-nlp-0.3.2/'\n",
    "\n",
    "PRONOUN = {}\n",
    "WHWORD = {}\n",
    "SLANG = {}\n",
    "INTENSIFIER = {}\n",
    "SUBJECTIVE = {}\n",
    "EVENT = {}\n",
    "MODAL = {}\n",
    "RELIGION = {}\n",
    "\n",
    "def READ_FILES():\n",
    "\n",
    "\tfp = open(PRONOUN_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tPRONOUN[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(INTENSIFIER_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tINTENSIFIER[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(WHWORD_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tWHWORD[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(SLANG_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tSLANG[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\tfp = open(EVENT_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tEVENT[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\t'''fp = open(NONSIT_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tEVENT[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close() '''\n",
    "\t\n",
    "\tfp = open(MODAL_VERB_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tMODAL[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(RELIGION_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tRELIGION[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\tfp = open(SUBJECTIVE_PATH,'r')\n",
    "        for l in fp:\n",
    "                wl = l.split()\n",
    "                x = wl[0].split('=')[1].strip(' \\t\\n\\r')\n",
    "                if x=='strongsubj':\n",
    "                        y = wl[2].split('=')[1].strip(' \\t\\n\\r')\n",
    "                        SUBJECTIVE[y.lower()] = 1\n",
    "\tfp.close()\n",
    "\n",
    "############################ This Functions are used #############################################\n",
    "\n",
    "def emoticons(s):\n",
    "        return len(re.findall(u'[\\U0001f600-\\U0001f60f\\U0001f617-\\U0001f61d\\U0001f632\\U0001f633\\U0001f638-\\U0001f63e\\U0001f642\\U0001f646-\\U0001f64f\\U0001f612\\U0001f613\\U0001f615\\U0001f616\\U0001f61e-\\U0001f629\\U0001f62c\\U0001f62d\\U0001f630\\U0001f631\\U0001f636\\U0001f637\\U0001f63c\\U0001f63f-\\U0001f641\\U0001f64d]', s))\n",
    "\n",
    "def smileys(s):\n",
    "        return len(re.findall(r':\\-\\)|:[\\)\\]\\}]|:[dDpP]|:3|:c\\)|:>|=\\]|8\\)|=\\)|:\\^\\)|:\\-D|[xX8]\\-?D|=\\-?D|=\\-?3|B\\^D|:\\'\\-?\\)|>:\\[|:\\-?\\(|:\\-?c|:\\-?<|:\\-?\\[|:\\{|;\\(|:\\-\\|\\||:@|>:\\(|:\\'\\-?\\(|D:<?|D[8;=X]|v.v|D\\-\\':|>:[\\/]|:\\-[./]|:[\\/LS]|=[\\/L]|>.<|:\\$|>:\\-?\\)|>;\\)|\\}:\\-?\\)|3:\\-?\\)|\\(>_<\\)>?|^_?^;|\\(#\\^.\\^#\\)|[Oo]_[Oo]|:\\-?o',s))\n",
    "\n",
    "def getNumberOfElongatedWords(s):\n",
    "    return len(re.findall('([a-zA-Z])\\\\1{2,}', s))\n",
    "    \n",
    "def pronoun(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif PRONOUN.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def exclamation(s):\n",
    "\tc = len(re.findall(r\"[!]\", s))\n",
    "\tif c>=1:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def question(s):\n",
    "\treturn len(re.findall(r\"[?]\", s))\n",
    "\n",
    "def intensifier(sen):\n",
    "\n",
    "\tcount = 0\n",
    "\tfor x in sen:\n",
    "\t\tif INTENSIFIER.__contains__(x)==True:\n",
    "\t\t\tcount+=1\n",
    "\t\t\t#return 1\n",
    "\tif count>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def whword(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif WHWORD.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def religion(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif RELIGION.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def slang(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif SLANG.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def event_phrase(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif EVENT.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def getHashtagopinion(sen):\n",
    "\tfp = codecs.open(OPINION_HASHTAG_PATH,'r','utf-8')\n",
    "\ttemp = set([])\n",
    "\tfor l in fp:\n",
    "\t\ttemp.add(l.strip(' \\t\\n\\r').lower())\n",
    "\tfp.close()\n",
    "\n",
    "\tcur_hash = set([])\n",
    "\tfor x in sen:\n",
    "\t\tif x.startswith('#')==True:\n",
    "\t\t\tcur_hash.add(x.strip(' \\t\\n\\r').lower())\n",
    "\tsize = len(temp.intersection(cur_hash))\n",
    "\tif size>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def numeral(temp):\n",
    "\tc = 0\n",
    "\tfor x in temp:\n",
    "\t\tif x.isdigit()==True:\n",
    "\t\t\tc+=1\n",
    "\treturn c\n",
    "\n",
    "def modal(sen):\n",
    "\tfor x in sen:\n",
    "\t\tif MODAL.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def subjectivity(sen):\n",
    "\t\n",
    "        c = 0\n",
    "        for x in sen:\n",
    "                if SUBJECTIVE.__contains__(x)==True:\n",
    "                        c+=1\n",
    "        tot = len(sen) + 4.0 - 4.0 - 1.0\n",
    "        num = c + 4.0 - 4.0\n",
    "\ttry:\n",
    "        \ts = round(num/tot,4)\n",
    "\texcept:\n",
    "\t\ts = 0\n",
    "\tif c>0:\n",
    "\t\treturn c\n",
    "\treturn 0\n",
    "\t#print(s)\n",
    "\t#return s\n",
    "\n",
    "#HashTagSentimentUnigrams(\"Oh wrestle\", \"NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#HashTagSentimentUnigrams(\"Oh no wrestle\", \"Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#print negatedContextCount(\"this is not for me\")\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "\n",
    "\tREAD_FILES()\n",
    "\ttok = Tokenizer(preserve_case=False)\n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\tfp = open(TRAIN1,'r')\n",
    "\tfor l in fp:\n",
    "\t     wl = l.split('\\t')\n",
    "\t     fo.write(wl[0].strip(' \\t\\n\\r') + '\\n')\n",
    "\tfp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\tfs = open(TRAIN1,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\thydb_feature = []\n",
    "\thydb_label = []\n",
    "\tfor l in fp:\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\trow = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(row[0])\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(row[0])\n",
    "\t\t\tQ = question(row[0])\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(row[0])\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\thydb_feature.append(t)\n",
    "\t\t\thydb_label.append(int(row[1]))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\tfs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "\n",
    "        hydb_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #hydb_clf = svm.SVC(kernel='rbf')\n",
    "        #hydb_clf = RandomForestClassifier()\n",
    "        #hydb_clf = svm.LinearSVC()\n",
    "        #hydb_clf = LogisticRegression()\n",
    "        #hydb_clf = BernoulliNB()\n",
    "        X_trainhy,X_testhy,y_trainhy,y_testhy=train_test_split(hydb_feature,hydb_label,test_size=0.25,random_state=1)\n",
    "        hydb_clf.fit(X_trainhy,y_trainhy)\n",
    "        y_pred_classhy=hydb_clf.predict(X_testhy)   \n",
    "        print metrics.classification_report(y_testhy, y_pred_classhy)\n",
    "        print metrics.accuracy_score(y_testhy,y_pred_classhy)\n",
    "        print roc_auc_score(y_testhy, y_pred_classhy)\n",
    "        print confusion_matrix(y_testhy,y_pred_classhy)\n",
    "        \n",
    "\tfo = open('temp.txt','w')\n",
    "\tfp = open(TRAIN2,'r')\n",
    "\tfor l in fp:\n",
    "\t     wl = l.split('\\t')\n",
    "\t     fo.write(wl[0].strip(' \\t\\n\\r') + '\\n')\n",
    "\tfp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\tfs = open(TRAIN2,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\tutkd_feature = []\n",
    "\tutkd_label = []\n",
    "\tfor l in fp:\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\trow = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(row[0])\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(row[0])\n",
    "\t\t\tQ = question(row[0])\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(row[0])\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\tutkd_feature.append(t)\n",
    "\t\t\tutkd_label.append(int(row[1]))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\tfs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "        \n",
    "\tutkd_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #utkd_clf = svm.SVC(kernel='rbf')\n",
    "        #utkd_clf = RandomForestClassifier()\n",
    "        #utkd_clf = svm.LinearSVC()\n",
    "        #utkd_clf = LogisticRegression()\n",
    "        #utkd_clf = BernoulliNB()\n",
    "        X_trainu,X_testu,y_trainu,y_testu=train_test_split(utkd_feature,utkd_label,test_size=0.25,random_state=1)\n",
    "        print len(X_testu),len(utkd_label)\n",
    "        utkd_clf.fit(X_trainu,y_trainu)\n",
    "        y_pred_classu=utkd_clf.predict(X_testu)   \n",
    "        print metrics.classification_report(y_testu, y_pred_classu)\n",
    "        print metrics.accuracy_score(y_testu,y_pred_classu)\n",
    "        print roc_auc_score(y_testu, y_pred_classu)\n",
    "        print confusion_matrix(y_testu,y_pred_classu)\n",
    "        \n",
    "\tfo = open('temp.txt','w')\n",
    "\tfp = open(TRAIN3,'r')\n",
    "\tfor l in fp:\n",
    "\t     wl = l.split('\\t')\n",
    "\t     fo.write(wl[0].strip(' \\t\\n\\r') + '\\n')\n",
    "\tfp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\tfs = open(TRAIN3,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\tsandy_hook_feature = []\n",
    "\tsandy_hook_label = []\n",
    "\tfor l in fp:\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\trow = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(row[0])\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(row[0])\n",
    "\t\t\tQ = question(row[0])\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(row[0])\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\tsandy_hook_feature.append(t)\n",
    "\t\t\tsandy_hook_label.append(int(row[1]))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\tfs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "\n",
    "        sandy_hook_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #sandy_hook_clf = svm.SVC(kernel='rbf')\n",
    "        #sandy_hook_clf = RandomForestClassifier()\n",
    "        #sandy_hook_clf = svm.LinearSVC()\n",
    "        #sandy_hook_clf = LogisticRegression()\n",
    "        #sandy_hook_clf = BernoulliNB()\n",
    "        X_trains,X_tests,y_trains,y_tests=train_test_split(sandy_hook_feature,sandy_hook_label,test_size=0.25,random_state=1)\n",
    "        print(X_tests)\n",
    "        sandy_hook_clf.fit(X_trains,y_trains)\n",
    "        y_pred_classs=sandy_hook_clf.predict(X_tests)   \n",
    "        print metrics.classification_report(y_tests, y_pred_classs)\n",
    "        print metrics.accuracy_score(y_tests,y_pred_classs)\n",
    "        print roc_auc_score(y_tests, y_pred_classs)\n",
    "        print confusion_matrix(y_tests,y_pred_classs)\n",
    "        \n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\tfp = open(TRAIN4,'r')\n",
    "\tfor l in fp:\n",
    "\t     wl = l.split('\\t')\n",
    "\t     fo.write(wl[0].strip(' \\t\\n\\r') + '\\n')\n",
    "\tfp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\tfs = open(TRAIN4,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\thydb_feature = []\n",
    "\thydb_label = []\n",
    "\tfor l in fp:\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\trow = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(row[0])\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(row[0])\n",
    "\t\t\tQ = question(row[0])\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(row[0])\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\thagupit_feature.append(t)\n",
    "\t\t\thagupit_label.append(int(row[1]))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\tfs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "        hagupit_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #hagupit_clf = svm.SVC(kernel='rbf')\n",
    "        #hagupit_clf = RandomForestClassifier()\n",
    "        #hagupit_clf = svm.LinearSVC()\n",
    "        #hagupit_clf = LogisticRegression()\n",
    "        #hagupit_clf = BernoulliNB()\n",
    "        X_trainh,X_testh,y_trainh,y_testh=train_test_split(hagupit_feature,hagupit_label,test_size=0.25,random_state=1)\n",
    "        print len(X_testh)\n",
    "        hagupit_clf.fit(X_trainh,y_trainh)\n",
    "        y_pred_classh=hagupit_clf.predict(X_testh)   \n",
    "        print metrics.classification_report(y_testh, y_pred_classh)\n",
    "        print metrics.accuracy_score(y_testh,y_pred_classh)\n",
    "        print roc_auc_score(y_testh, y_pred_classh)\n",
    "        print confusion_matrix(y_testh,y_pred_classh)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing on different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Testing the same dataset\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.60      0.68       209\n",
      "          1       0.68      0.83      0.75       215\n",
      "\n",
      "avg / total       0.73      0.72      0.71       424\n",
      "\n",
      "0.7169811320754716\n",
      "0.7153221319683989\n",
      "[[125  84]\n",
      " [ 36 179]]\n",
      "Training Smerp level1 and Testing level2\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.58      0.64      1170\n",
      "          1       0.65      0.77      0.70      1171\n",
      "\n",
      "avg / total       0.68      0.67      0.67      2341\n",
      "\n",
      "0.6744980777445536\n",
      "0.6744564146357485\n",
      "[[675 495]\n",
      " [267 904]]\n",
      "Traning SMERPlevel-1 and Testing FIRE\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hybd_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fc8f1c0786fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;31m#sandy_hook_clf.fit(X_trains,y_trains)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Traning SMERPlevel-1 and Testing FIRE\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0my_pred_classs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhybd_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msandhy_hook_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msandy_hook_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_classs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msandy_hook_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred_classs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hybd_clf' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on 11-May-2015\n",
    "\n",
    "@author: Koustav\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "from textblob import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn import svm\n",
    "from sklearn.svm import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import gzip\n",
    "from happyfuntokenizing import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import *\n",
    "#from happyfuntokenizing import *\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "mycompile = lambda pat:  re.compile(pat,  re.UNICODE)\n",
    "\n",
    "PRONOUN_PATH = '../../Lexical_resources/classifier_dictionary/english_pronoun.txt'\n",
    "WHWORD_PATH = '../classifier_dictionary/english_whwords.txt'\n",
    "SLANG_PATH = '../classifier_dictionary/english_swear.txt'\n",
    "INTENSIFIER_PATH = '../classifier_dictionary/english_intensifier.txt'\n",
    "SUBJECTIVE_PATH = '../classifier_dictionary/subjclueslen1-HLTEMNLP05.tff'\n",
    "EVENT_PATH = '../classifier_dictionary/english_nonsituational_phrase.txt'\n",
    "MODAL_VERB_PATH = '../classifier_dictionary/english_modal_verb.txt'\n",
    "RELIGION_PATH = '../classifier_dictionary/communal_race.txt'\n",
    "#NONSIT_PATH = 'Common_nonsituational_word.txt'\n",
    "#OPINION_HASHTAG_PATH = '/home/krudra/twitter_code/shared/language_overlap/code_mix_pitch/devanagari/devanagari_hashtag_opinion.txt'\n",
    "#MENTION_PATH = '/home/krudra/twitter_code/shared/language_overlap/code_mix_pitch/devanagari/news_mention.txt'\n",
    "\n",
    "#TRAIN1 = '../classification_4.3/fragmented_train_data/hydb_fragment_train.txt'\n",
    "#TRAIN2 = '../classification_4.3/fragmented_train_data/utkd_fragment_train.txt'\n",
    "#TRAIN3 = '../classification_4.3/fragmented_train_data/sandy_hook_fragment_train.txt'\n",
    "#TRAIN4 = '../classification_4.3/fragmented_train_data/hagupit_fragment_train.txt'\n",
    "TRAIN2=\"../dataset4may/SMERP/RetrievedSMERP/level-1/TotalSMERP_T31.tsv\"\n",
    "TRAIN1=\"smerpl2.tsv\"\n",
    "TRAIN3=\"fire.tsv\"\n",
    "TRAIN4=\"Nepal.tsv\"\n",
    "\n",
    "'''TRAIN1 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hydb_RAW_CLASS.txt'\n",
    "TRAIN2 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/utkd_RAW_CLASS.txt'\n",
    "TRAIN3 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/sandy_hook_RAW_CLASS.txt'\n",
    "TRAIN4 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hagupit_RAW_CLASS.txt' '''\n",
    "\n",
    "'''TRAIN1 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hydb_balance_RAW.txt'\n",
    "TRAIN2 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/utkd_balance_RAW.txt'\n",
    "TRAIN3 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/sandy_hook_balance_RAW.txt'\n",
    "TRAIN4 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hagupit_balance_RAW.txt' '''\n",
    "\n",
    "\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "TAGGER_PATH = '../../Lexical_resources/ark-tweet-nlp-0.3.2/'\n",
    "\n",
    "PRONOUN = {}\n",
    "WHWORD = {}\n",
    "SLANG = {}\n",
    "INTENSIFIER = {}\n",
    "SUBJECTIVE = {}\n",
    "EVENT = {}\n",
    "MODAL = {}\n",
    "RELIGION = {}\n",
    "\n",
    "def READ_FILES():\n",
    "\n",
    "\tfp = open(PRONOUN_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tPRONOUN[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(INTENSIFIER_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tINTENSIFIER[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(WHWORD_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tWHWORD[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(SLANG_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tSLANG[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\tfp = open(EVENT_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tEVENT[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\t'''fp = open(NONSIT_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tEVENT[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close() '''\n",
    "\t\n",
    "\tfp = open(MODAL_VERB_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tMODAL[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(RELIGION_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tRELIGION[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\tfp = open(SUBJECTIVE_PATH,'r')\n",
    "        for l in fp:\n",
    "                wl = l.split()\n",
    "                x = wl[0].split('=')[1].strip(' \\t\\n\\r')\n",
    "                if x=='strongsubj':\n",
    "                        y = wl[2].split('=')[1].strip(' \\t\\n\\r')\n",
    "                        SUBJECTIVE[y.lower()] = 1\n",
    "\tfp.close()\n",
    "\n",
    "############################ This Functions are used #############################################\n",
    "\n",
    "def emoticons(s):\n",
    "        return len(re.findall(u'[\\U0001f600-\\U0001f60f\\U0001f617-\\U0001f61d\\U0001f632\\U0001f633\\U0001f638-\\U0001f63e\\U0001f642\\U0001f646-\\U0001f64f\\U0001f612\\U0001f613\\U0001f615\\U0001f616\\U0001f61e-\\U0001f629\\U0001f62c\\U0001f62d\\U0001f630\\U0001f631\\U0001f636\\U0001f637\\U0001f63c\\U0001f63f-\\U0001f641\\U0001f64d]', s))\n",
    "\n",
    "def smileys(s):\n",
    "        return len(re.findall(r':\\-\\)|:[\\)\\]\\}]|:[dDpP]|:3|:c\\)|:>|=\\]|8\\)|=\\)|:\\^\\)|:\\-D|[xX8]\\-?D|=\\-?D|=\\-?3|B\\^D|:\\'\\-?\\)|>:\\[|:\\-?\\(|:\\-?c|:\\-?<|:\\-?\\[|:\\{|;\\(|:\\-\\|\\||:@|>:\\(|:\\'\\-?\\(|D:<?|D[8;=X]|v.v|D\\-\\':|>:[\\/]|:\\-[./]|:[\\/LS]|=[\\/L]|>.<|:\\$|>:\\-?\\)|>;\\)|\\}:\\-?\\)|3:\\-?\\)|\\(>_<\\)>?|^_?^;|\\(#\\^.\\^#\\)|[Oo]_[Oo]|:\\-?o',s))\n",
    "\n",
    "def getNumberOfElongatedWords(s):\n",
    "    return len(re.findall('([a-zA-Z])\\\\1{2,}', s))\n",
    "    \n",
    "def pronoun(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif PRONOUN.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def exclamation(s):\n",
    "\tc = len(re.findall(r\"[!]\", s))\n",
    "\tif c>=1:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def question(s):\n",
    "\treturn len(re.findall(r\"[?]\", s))\n",
    "\n",
    "def intensifier(sen):\n",
    "\n",
    "\tcount = 0\n",
    "\tfor x in sen:\n",
    "\t\tif INTENSIFIER.__contains__(x)==True:\n",
    "\t\t\tcount+=1\n",
    "\t\t\t#return 1\n",
    "\tif count>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def whword(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif WHWORD.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def religion(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif RELIGION.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def slang(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif SLANG.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def event_phrase(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif EVENT.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def getHashtagopinion(sen):\n",
    "\tfp = codecs.open(OPINION_HASHTAG_PATH,'r','utf-8')\n",
    "\ttemp = set([])\n",
    "\tfor l in fp:\n",
    "\t\ttemp.add(l.strip(' \\t\\n\\r').lower())\n",
    "\tfp.close()\n",
    "\n",
    "\tcur_hash = set([])\n",
    "\tfor x in sen:\n",
    "\t\tif x.startswith('#')==True:\n",
    "\t\t\tcur_hash.add(x.strip(' \\t\\n\\r').lower())\n",
    "\tsize = len(temp.intersection(cur_hash))\n",
    "\tif size>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def numeral(temp):\n",
    "\tc = 0\n",
    "\tfor x in temp:\n",
    "\t\tif x.isdigit()==True:\n",
    "\t\t\tc+=1\n",
    "\treturn c\n",
    "\n",
    "def modal(sen):\n",
    "\tfor x in sen:\n",
    "\t\tif MODAL.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def subjectivity(sen):\n",
    "\t\n",
    "        c = 0\n",
    "        for x in sen:\n",
    "                if SUBJECTIVE.__contains__(x)==True:\n",
    "                        c+=1\n",
    "        tot = len(sen) + 4.0 - 4.0 - 1.0\n",
    "        num = c + 4.0 - 4.0\n",
    "\ttry:\n",
    "        \ts = round(num/tot,4)\n",
    "\texcept:\n",
    "\t\ts = 0\n",
    "\tif c>0:\n",
    "\t\treturn c\n",
    "\treturn 0\n",
    "\t#print(s)\n",
    "\t#return s\n",
    "\n",
    "#HashTagSentimentUnigrams(\"Oh wrestle\", \"NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#HashTagSentimentUnigrams(\"Oh no wrestle\", \"Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#print negatedContextCount(\"this is not for me\")\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "\n",
    "\tREAD_FILES()\n",
    "\ttok = Tokenizer(preserve_case=False)\n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\tfp = open(TRAIN1,'r')\n",
    "\tfor l in fp:\n",
    "\t     wl = l.split('\\t')\n",
    "\t     fo.write(wl[0].strip(' \\t\\n\\r') + '\\n')\n",
    "\tfp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\tfs = open(TRAIN1,'r')\n",
    "\ts = ''\n",
    "\tN = 0    \n",
    "\thydb_feature = []\n",
    "\thydb_label = []\n",
    "\tfor l in fp:\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\trow = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(row[0])\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(row[0])\n",
    "\t\t\tQ = question(row[0])\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(row[0])\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\thydb_feature.append(t)\n",
    "\t\t\thydb_label.append(int(row[1]))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\tfs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "\n",
    "\thydb_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #hydb_clf = svm.SVC(kernel='rbf')\n",
    "        #hydb_clf = RandomForestClassifier()\n",
    "        #hydb_clf = svm.LinearSVC()\n",
    "        #hydb_clf = LogisticRegression()\n",
    "        #hydb_clf = BernoulliNB()\n",
    "        X_trainhy,X_testhy,y_trainhy,y_testhy=train_test_split(hydb_feature,hydb_label,test_size=0.25,random_state=1)\n",
    "        hydb_clf.fit(X_trainhy,y_trainhy)\n",
    "        y_pred_classhy=hydb_clf.predict(X_testhy) \n",
    "        print \"Training and Testing the same dataset\"\n",
    "        print metrics.classification_report(y_testhy, y_pred_classhy)\n",
    "        print metrics.accuracy_score(y_testhy, y_pred_classhy)\n",
    "        print roc_auc_score(y_testhy, y_pred_classhy)\n",
    "        print confusion_matrix(y_testhy, y_pred_classhy)\n",
    "         \n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\tfp = open(TRAIN2,'r')\n",
    "\tfor l in fp:\n",
    "\t     wl = l.split('\\t')\n",
    "\t     fo.write(wl[0].strip(' \\t\\n\\r') + '\\n')\n",
    "\tfp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\tfs = open(TRAIN2,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\tglobal utkd_feature \n",
    "\tglobal utkd_label\n",
    "\tutkd_feature = []\n",
    "\tutkd_label = []\n",
    "\tfor l in fp:\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\trow = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(row[0])\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(row[0])\n",
    "\t\t\tQ = question(row[0])\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(row[0])\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\tutkd_feature.append(t)\n",
    "\t\t\tutkd_label.append(int(row[1]))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\tfs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "        \n",
    "\tutkd_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #utkd_clf = svm.SVC(kernel='rbf')\n",
    "        #utkd_clf = RandomForestClassifier()\n",
    "        #utkd_clf = svm.LinearSVC()\n",
    "        #utkd_clf = LogisticRegression()\n",
    "        #utkd_clf = BernoulliNB()\n",
    "        #X_trainu,X_testu,y_trainu,y_testu=train_test_split(utkd_feature,utkd_label,test_size=0.25,random_state=1)\n",
    "        utkd_clf.fit(utkd_feature,utkd_label)\n",
    "        y_pred_classu=hydb_clf.predict(utkd_feature) \n",
    "        print \"Training Smerp level1 and Testing level2\"\n",
    "        print metrics.classification_report(utkd_label, y_pred_classu)\n",
    "        print metrics.accuracy_score(utkd_label,y_pred_classu)\n",
    "        print roc_auc_score(utkd_label, y_pred_classu)\n",
    "        print confusion_matrix(utkd_label,y_pred_classu)\n",
    "        #print \"Training Smerp level2 and Testing level1\"\n",
    "        #y_pred_classu2=utkd_clf.predict(hybd_feature) \n",
    "        #print metrics.classification_report(utkd_label, y_pred_classu2)\n",
    "        #print metrics.accuracy_score(utkd_label,y_pred_classu2)\n",
    "        #print roc_auc_score(utkd_label, y_pred_classu2)\n",
    "        #print confusion_matrix(utkd_label,y_pred_classu2)\n",
    "        \n",
    "\tfo = open('temp.txt','w')\n",
    "\tfp = open(TRAIN3,'r')\n",
    "\tfor l in fp:\n",
    "\t     wl = l.split('\\t')\n",
    "\t     fo.write(wl[0].strip(' \\t\\n\\r') + '\\n')\n",
    "\tfp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\tfs = open(TRAIN3,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\tsandy_hook_feature = []\n",
    "\tsandy_hook_label = []\n",
    "\tfor l in fp:\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\trow = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(row[0])\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(row[0])\n",
    "\t\t\tQ = question(row[0])\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(row[0])\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\tsandy_hook_feature.append(t)\n",
    "\t\t\tsandy_hook_label.append(int(row[1]))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\tfs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "\n",
    "        sandy_hook_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #sandy_hook_clf = svm.SVC(kernel='rbf')\n",
    "        #sandy_hook_clf = RandomForestClassifier()\n",
    "        #sandy_hook_clf = svm.LinearSVC()\n",
    "        #sandy_hook_clf = LogisticRegression()\n",
    "        #sandy_hook_clf = BernoulliNB()\n",
    "        #X_trains,X_tests,y_trains,y_tests=train_test_split(sandy_hook_feature,sandy_hook_label,test_size=0.25,random_state=1)\n",
    "        #print(X_tests)\n",
    "        #sandy_hook_clf.fit(X_trains,y_trains)\n",
    "        print \"Traning SMERPlevel-1 and Testing FIRE\"\n",
    "        y_pred_classs=hybd_clf.predict(sandhy_hook_feature)   \n",
    "        print metrics.classification_report(sandy_hook_label, y_pred_classs)\n",
    "        print metrics.accuracy_score(sandy_hook_label,y_pred_classs)\n",
    "        print roc_auc_score(sandy_hook_label, y_pred_classs)\n",
    "        print confusion_matrix(sandy_hook_label,y_pred_classs)\n",
    "        #print \"Traning SMERPlevel-2 and Testing FIRE\"\n",
    "        #y_pred_class2=utkd_clf.predict(sandhy_hook_feature)   \n",
    "       # print metrics.classification_report(sandy_hook_label, y_pred_class2)\n",
    "        #print metrics.accuracy_score(sandy_hook_label,y_pred_class2)\n",
    "        #print roc_auc_score(sandy_hook_label, y_pred_class2)\n",
    "        #print confusion_matrix(sandy_hook_label,y_pred_class2)\n",
    "        \n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\tfp = open(TRAIN4,'r')\n",
    "\tfor l in fp:\n",
    "\t     wl = l.split('\\t')\n",
    "\t     fo.write(wl[0].strip(' \\t\\n\\r') + '\\n')\n",
    "\tfp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\tfs = open(TRAIN4,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\thydb_feature = []\n",
    "\thydb_label = []\n",
    "\tfor l in fp:\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\trow = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(row[0])\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(row[0])\n",
    "\t\t\tQ = question(row[0])\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(row[0])\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\thagupit_feature.append(t)\n",
    "\t\t\thagupit_label.append(int(row[1]))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\tfs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "        hagupit_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #hagupit_clf = svm.SVC(kernel='rbf')\n",
    "        #hagupit_clf = RandomForestClassifier()\n",
    "        #hagupit_clf = svm.LinearSVC()\n",
    "        #hagupit_clf = LogisticRegression()\n",
    "        #hagupit_clf = BernoulliNB()\n",
    "        #X_trainh,X_testh,y_trainh,y_testh=train_test_split(hagupit_feature,hagupit_label,test_size=0.25,random_state=1)\n",
    "       # print len(X_testh)\n",
    "       # hagupit_clf.fit(X_trainh,y_trainh)\n",
    "        print \"Training smerpl-1 and testing nepal\"\n",
    "        y_pred_classh=hybd_clf.predict(hagupit_label)   \n",
    "        print metrics.classification_report(hagupit_label, y_pred_classh)\n",
    "        print metrics.accuracy_score(hagupit_label,y_pred_classh)\n",
    "        print roc_auc_score(hagupit_label, y_pred_classh)\n",
    "        print confusion_matrix(hagupit_label,y_pred_classh)  \n",
    "        #print \"Training smerpl-2 and testing nepal\"\n",
    "        #y_pred_classh2=utkd_clf.predict(hagupit_label)   \n",
    "        #print metrics.classification_report(hagupit_label, y_pred_classh2)\n",
    "        #print metrics.accuracy_score(hagupit_label,y_pred_classh2)\n",
    "       # print roc_auc_score(hagupit_label, y_pred_classh2)\n",
    "        #print confusion_matrix(hagupit_label,y_pred_classh2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Smerp level1 and Testing level2\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.43      0.44       302\n",
      "          1       0.46      0.49      0.48       302\n",
      "\n",
      "avg / total       0.46      0.46      0.46       604\n",
      "\n",
      "0.4602649006622517\n",
      "0.4602649006622517\n",
      "[[130 172]\n",
      " [154 148]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on 11-May-2015\n",
    "\n",
    "@author: Koustav\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "from textblob import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn import svm\n",
    "from sklearn.svm import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import gzip\n",
    "from happyfuntokenizing import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import *\n",
    "#from happyfuntokenizing import *\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "mycompile = lambda pat:  re.compile(pat,  re.UNICODE)\n",
    "\n",
    "PRONOUN_PATH = '../../Lexical_resources/classifier_dictionary/english_pronoun.txt'\n",
    "WHWORD_PATH = '../classifier_dictionary/english_whwords.txt'\n",
    "SLANG_PATH = '../classifier_dictionary/english_swear.txt'\n",
    "INTENSIFIER_PATH = '../classifier_dictionary/english_intensifier.txt'\n",
    "SUBJECTIVE_PATH = '../classifier_dictionary/subjclueslen1-HLTEMNLP05.tff'\n",
    "EVENT_PATH = '../classifier_dictionary/english_nonsituational_phrase.txt'\n",
    "MODAL_VERB_PATH = '../classifier_dictionary/english_modal_verb.txt'\n",
    "RELIGION_PATH = '../classifier_dictionary/communal_race.txt'\n",
    "#NONSIT_PATH = 'Common_nonsituational_word.txt'\n",
    "#OPINION_HASHTAG_PATH = '/home/krudra/twitter_code/shared/language_overlap/code_mix_pitch/devanagari/devanagari_hashtag_opinion.txt'\n",
    "#MENTION_PATH = '/home/krudra/twitter_code/shared/language_overlap/code_mix_pitch/devanagari/news_mention.txt'\n",
    "\n",
    "#TRAIN1 = '../classification_4.3/fragmented_train_data/hydb_fragment_train.txt'\n",
    "#TRAIN2 = '../classification_4.3/fragmented_train_data/utkd_fragment_train.txt'\n",
    "#TRAIN3 = '../classification_4.3/fragmented_train_data/sandy_hook_fragment_train.txt'\n",
    "#TRAIN4 = '../classification_4.3/fragmented_train_data/hagupit_fragment_train.txt'\n",
    "TRAIN1=\"../dataset4may/SMERP/RetrievedSMERP/level-1/TotalSMERP_T31.tsv\"\n",
    "TRAIN2=\"smerpl2.tsv\"\n",
    "TRAIN3=\"fire.tsv\"\n",
    "TRAIN4=\"Nepal.tsv\"\n",
    "\n",
    "'''TRAIN1 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hydb_RAW_CLASS.txt'\n",
    "TRAIN2 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/utkd_RAW_CLASS.txt'\n",
    "TRAIN3 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/sandy_hook_RAW_CLASS.txt'\n",
    "TRAIN4 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hagupit_RAW_CLASS.txt' '''\n",
    "\n",
    "'''TRAIN1 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hydb_balance_RAW.txt'\n",
    "TRAIN2 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/utkd_balance_RAW.txt'\n",
    "TRAIN3 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/sandy_hook_balance_RAW.txt'\n",
    "TRAIN4 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hagupit_balance_RAW.txt' '''\n",
    "\n",
    "\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "TAGGER_PATH = '../../Lexical_resources/ark-tweet-nlp-0.3.2/'\n",
    "\n",
    "PRONOUN = {}\n",
    "WHWORD = {}\n",
    "SLANG = {}\n",
    "INTENSIFIER = {}\n",
    "SUBJECTIVE = {}\n",
    "EVENT = {}\n",
    "MODAL = {}\n",
    "RELIGION = {}\n",
    "\n",
    "def READ_FILES():\n",
    "\n",
    "\tfp = open(PRONOUN_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tPRONOUN[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(INTENSIFIER_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tINTENSIFIER[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(WHWORD_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tWHWORD[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(SLANG_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tSLANG[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\tfp = open(EVENT_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tEVENT[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\t'''fp = open(NONSIT_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tEVENT[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close() '''\n",
    "\t\n",
    "\tfp = open(MODAL_VERB_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tMODAL[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(RELIGION_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tRELIGION[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\tfp = open(SUBJECTIVE_PATH,'r')\n",
    "        for l in fp:\n",
    "                wl = l.split()\n",
    "                x = wl[0].split('=')[1].strip(' \\t\\n\\r')\n",
    "                if x=='strongsubj':\n",
    "                        y = wl[2].split('=')[1].strip(' \\t\\n\\r')\n",
    "                        SUBJECTIVE[y.lower()] = 1\n",
    "\tfp.close()\n",
    "\n",
    "############################ This Functions are used #############################################\n",
    "\n",
    "def emoticons(s):\n",
    "        return len(re.findall(u'[\\U0001f600-\\U0001f60f\\U0001f617-\\U0001f61d\\U0001f632\\U0001f633\\U0001f638-\\U0001f63e\\U0001f642\\U0001f646-\\U0001f64f\\U0001f612\\U0001f613\\U0001f615\\U0001f616\\U0001f61e-\\U0001f629\\U0001f62c\\U0001f62d\\U0001f630\\U0001f631\\U0001f636\\U0001f637\\U0001f63c\\U0001f63f-\\U0001f641\\U0001f64d]', s))\n",
    "\n",
    "def smileys(s):\n",
    "        return len(re.findall(r':\\-\\)|:[\\)\\]\\}]|:[dDpP]|:3|:c\\)|:>|=\\]|8\\)|=\\)|:\\^\\)|:\\-D|[xX8]\\-?D|=\\-?D|=\\-?3|B\\^D|:\\'\\-?\\)|>:\\[|:\\-?\\(|:\\-?c|:\\-?<|:\\-?\\[|:\\{|;\\(|:\\-\\|\\||:@|>:\\(|:\\'\\-?\\(|D:<?|D[8;=X]|v.v|D\\-\\':|>:[\\/]|:\\-[./]|:[\\/LS]|=[\\/L]|>.<|:\\$|>:\\-?\\)|>;\\)|\\}:\\-?\\)|3:\\-?\\)|\\(>_<\\)>?|^_?^;|\\(#\\^.\\^#\\)|[Oo]_[Oo]|:\\-?o',s))\n",
    "\n",
    "def getNumberOfElongatedWords(s):\n",
    "    return len(re.findall('([a-zA-Z])\\\\1{2,}', s))\n",
    "    \n",
    "def pronoun(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif PRONOUN.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def exclamation(s):\n",
    "\tc = len(re.findall(r\"[!]\", s))\n",
    "\tif c>=1:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def question(s):\n",
    "\treturn len(re.findall(r\"[?]\", s))\n",
    "\n",
    "def intensifier(sen):\n",
    "\n",
    "\tcount = 0\n",
    "\tfor x in sen:\n",
    "\t\tif INTENSIFIER.__contains__(x)==True:\n",
    "\t\t\tcount+=1\n",
    "\t\t\t#return 1\n",
    "\tif count>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def whword(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif WHWORD.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def religion(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif RELIGION.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def slang(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif SLANG.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def event_phrase(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif EVENT.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def getHashtagopinion(sen):\n",
    "\tfp = codecs.open(OPINION_HASHTAG_PATH,'r','utf-8')\n",
    "\ttemp = set([])\n",
    "\tfor l in fp:\n",
    "\t\ttemp.add(l.strip(' \\t\\n\\r').lower())\n",
    "\tfp.close()\n",
    "\n",
    "\tcur_hash = set([])\n",
    "\tfor x in sen:\n",
    "\t\tif x.startswith('#')==True:\n",
    "\t\t\tcur_hash.add(x.strip(' \\t\\n\\r').lower())\n",
    "\tsize = len(temp.intersection(cur_hash))\n",
    "\tif size>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def numeral(temp):\n",
    "\tc = 0\n",
    "\tfor x in temp:\n",
    "\t\tif x.isdigit()==True:\n",
    "\t\t\tc+=1\n",
    "\treturn c\n",
    "\n",
    "def modal(sen):\n",
    "\tfor x in sen:\n",
    "\t\tif MODAL.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def subjectivity(sen):\n",
    "\t\n",
    "        c = 0\n",
    "        for x in sen:\n",
    "                if SUBJECTIVE.__contains__(x)==True:\n",
    "                        c+=1\n",
    "        tot = len(sen) + 4.0 - 4.0 - 1.0\n",
    "        num = c + 4.0 - 4.0\n",
    "\ttry:\n",
    "        \ts = round(num/tot,4)\n",
    "\texcept:\n",
    "\t\ts = 0\n",
    "\tif c>0:\n",
    "\t\treturn c\n",
    "\treturn 0\n",
    "\t#print(s)\n",
    "\t#return s\n",
    "\n",
    "#HashTagSentimentUnigrams(\"Oh wrestle\", \"NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#HashTagSentimentUnigrams(\"Oh no wrestle\", \"Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#print negatedContextCount(\"this is not for me\")\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "\n",
    "\tREAD_FILES()\n",
    "\ttok = Tokenizer(preserve_case=False)\n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\tfp = open(TRAIN2,'r')\n",
    "\tfor l in fp:\n",
    "\t     wl = l.split('\\t')\n",
    "\t     fo.write(wl[0].strip(' \\t\\n\\r') + '\\n')\n",
    "\tfp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\tfs = open(TRAIN2,'r')\n",
    "\ts = ''\n",
    "\tN = 0    \n",
    "\thydb_feature = []\n",
    "\thydb_label = []\n",
    "\tfor l in fp:\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\trow = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(row[0])\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(row[0])\n",
    "\t\t\tQ = question(row[0])\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(row[0])\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\thydb_feature.append(t)\n",
    "\t\t\thydb_label.append(int(row[1]))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\tfs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "\n",
    "\thydb_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #hydb_clf = svm.SVC(kernel='rbf')\n",
    "        #hydb_clf = RandomForestClassifier()\n",
    "        #hydb_clf = svm.LinearSVC()\n",
    "        #hydb_clf = LogisticRegression()\n",
    "        #hydb_clf = BernoulliNB()\n",
    "        #X_trainhy,X_testhy,y_trainhy,y_testhy=train_test_split(hydb_feature,hydb_label,test_size=0.25,random_state=1)\n",
    "        hydb_clf.fit(hydb_feature,hydb_label)\n",
    "         \n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\tfp = open(TRAIN4,'r')\n",
    "\tfor l in fp:\n",
    "\t     wl = l.split('\\t')\n",
    "\t     fo.write(wl[0].strip(' \\t\\n\\r') + '\\n')\n",
    "\tfp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\tfs = open(TRAIN4,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\tutkd_feature = []\n",
    "\tutkd_label = []\n",
    "\tfor l in fp:\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\trow = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(row[0])\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(row[0])\n",
    "\t\t\tQ = question(row[0])\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(row[0])\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\tutkd_feature.append(t)\n",
    "\t\t\tutkd_label.append(int(row[1]))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\tfs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "        \n",
    "\tutkd_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #utkd_clf = svm.SVC(kernel='rbf')\n",
    "        #utkd_clf = RandomForestClassifier()\n",
    "        #utkd_clf = svm.LinearSVC()\n",
    "        #utkd_clf = LogisticRegression()\n",
    "        #utkd_clf = BernoulliNB()\n",
    "        #X_trainu,X_testu,y_trainu,y_testu=train_test_split(utkd_feature,utkd_label,test_size=0.25,random_state=1)\n",
    "        utkd_clf.fit(utkd_feature,utkd_label)\n",
    "        y_pred_classu=hydb_clf.predict(utkd_feature) \n",
    "        print \"Training Smerp level1 and Testing level2\"\n",
    "        print metrics.classification_report(utkd_label, y_pred_classu)\n",
    "        print metrics.accuracy_score(utkd_label,y_pred_classu)\n",
    "        print roc_auc_score(utkd_label, y_pred_classu)\n",
    "        print confusion_matrix(utkd_label,y_pred_classu)\n",
    "        #print \"Training Smerp level2 and Testing level1\"\n",
    "        #y_pred_classu2=utkd_clf.predict(hybd_feature) \n",
    "        #print metrics.classification_report(utkd_label, y_pred_classu2)\n",
    "        #print metrics.accuracy_score(utkd_label,y_pred_classu2)\n",
    "        #print roc_auc_score(utkd_label, y_pred_classu2)\n",
    "        #print confusion_matrix(utkd_label,y_pred_classu2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BY USING X_train, X_test, y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Smerp level1 and Testing level2\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.46      0.55        46\n",
      "          1       0.45      0.80      0.58        46\n",
      "          2       0.28      0.15      0.20        46\n",
      "\n",
      "avg / total       0.47      0.47      0.44       138\n",
      "\n",
      "0.47101449275362317\n",
      "[[21 14 11]\n",
      " [ 2 37  7]\n",
      " [ 8 31  7]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on 11-May-2015\n",
    "\n",
    "@author: Koustav\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "from textblob import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn import svm\n",
    "from sklearn.svm import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import gzip\n",
    "from happyfuntokenizing import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import *\n",
    "#from happyfuntokenizing import *\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "mycompile = lambda pat:  re.compile(pat,  re.UNICODE)\n",
    "\n",
    "PRONOUN_PATH = '../../Lexical_resources/classifier_dictionary/english_pronoun.txt'\n",
    "WHWORD_PATH = '../classifier_dictionary/english_whwords.txt'\n",
    "SLANG_PATH = '../classifier_dictionary/english_swear.txt'\n",
    "INTENSIFIER_PATH = '../classifier_dictionary/english_intensifier.txt'\n",
    "SUBJECTIVE_PATH = '../classifier_dictionary/subjclueslen1-HLTEMNLP05.tff'\n",
    "EVENT_PATH = '../classifier_dictionary/english_nonsituational_phrase.txt'\n",
    "MODAL_VERB_PATH = '../classifier_dictionary/english_modal_verb.txt'\n",
    "RELIGION_PATH = '../classifier_dictionary/communal_race.txt'\n",
    "#NONSIT_PATH = 'Common_nonsituational_word.txt'\n",
    "#OPINION_HASHTAG_PATH = '/home/krudra/twitter_code/shared/language_overlap/code_mix_pitch/devanagari/devanagari_hashtag_opinion.txt'\n",
    "#MENTION_PATH = '/home/krudra/twitter_code/shared/language_overlap/code_mix_pitch/devanagari/news_mention.txt'\n",
    "\n",
    "#TRAIN1 = '../classification_4.3/fragmented_train_data/hydb_fragment_train.txt'\n",
    "#TRAIN2 = '../classification_4.3/fragmented_train_data/utkd_fragment_train.txt'\n",
    "#TRAIN3 = '../classification_4.3/fragmented_train_data/sandy_hook_fragment_train.txt'\n",
    "#TRAIN4 = '../classification_4.3/fragmented_train_data/hagupit_fragment_train.txt'\n",
    "TRAIN1=\"../dataset4may/SMERP/RetrievedSMERP/level-1/TotalSMERP_T31.tsv\"\n",
    "TRAIN2=\"smerpl2.tsv\"\n",
    "TRAIN3=\"fire.tsv\"\n",
    "TRAIN4=\"Nepal.tsv\"\n",
    "\n",
    "'''TRAIN1 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hydb_RAW_CLASS.txt'\n",
    "TRAIN2 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/utkd_RAW_CLASS.txt'\n",
    "TRAIN3 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/sandy_hook_RAW_CLASS.txt'\n",
    "TRAIN4 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hagupit_RAW_CLASS.txt' '''\n",
    "\n",
    "'''TRAIN1 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hydb_balance_RAW.txt'\n",
    "TRAIN2 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/utkd_balance_RAW.txt'\n",
    "TRAIN3 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/sandy_hook_balance_RAW.txt'\n",
    "TRAIN4 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hagupit_balance_RAW.txt' '''\n",
    "\n",
    "\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "TAGGER_PATH = '../../Lexical_resources/ark-tweet-nlp-0.3.2/'\n",
    "\n",
    "PRONOUN = {}\n",
    "WHWORD = {}\n",
    "SLANG = {}\n",
    "INTENSIFIER = {}\n",
    "SUBJECTIVE = {}\n",
    "EVENT = {}\n",
    "MODAL = {}\n",
    "RELIGION = {}\n",
    "\n",
    "def READ_FILES():\n",
    "\n",
    "\tfp = open(PRONOUN_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tPRONOUN[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(INTENSIFIER_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tINTENSIFIER[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(WHWORD_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tWHWORD[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(SLANG_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tSLANG[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\tfp = open(EVENT_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tEVENT[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\t'''fp = open(NONSIT_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tEVENT[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close() '''\n",
    "\t\n",
    "\tfp = open(MODAL_VERB_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tMODAL[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(RELIGION_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tRELIGION[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\tfp = open(SUBJECTIVE_PATH,'r')\n",
    "        for l in fp:\n",
    "                wl = l.split()\n",
    "                x = wl[0].split('=')[1].strip(' \\t\\n\\r')\n",
    "                if x=='strongsubj':\n",
    "                        y = wl[2].split('=')[1].strip(' \\t\\n\\r')\n",
    "                        SUBJECTIVE[y.lower()] = 1\n",
    "\tfp.close()\n",
    "\n",
    "############################ This Functions are used #############################################\n",
    "\n",
    "def emoticons(s):\n",
    "        return len(re.findall(u'[\\U0001f600-\\U0001f60f\\U0001f617-\\U0001f61d\\U0001f632\\U0001f633\\U0001f638-\\U0001f63e\\U0001f642\\U0001f646-\\U0001f64f\\U0001f612\\U0001f613\\U0001f615\\U0001f616\\U0001f61e-\\U0001f629\\U0001f62c\\U0001f62d\\U0001f630\\U0001f631\\U0001f636\\U0001f637\\U0001f63c\\U0001f63f-\\U0001f641\\U0001f64d]', s))\n",
    "\n",
    "def smileys(s):\n",
    "        return len(re.findall(r':\\-\\)|:[\\)\\]\\}]|:[dDpP]|:3|:c\\)|:>|=\\]|8\\)|=\\)|:\\^\\)|:\\-D|[xX8]\\-?D|=\\-?D|=\\-?3|B\\^D|:\\'\\-?\\)|>:\\[|:\\-?\\(|:\\-?c|:\\-?<|:\\-?\\[|:\\{|;\\(|:\\-\\|\\||:@|>:\\(|:\\'\\-?\\(|D:<?|D[8;=X]|v.v|D\\-\\':|>:[\\/]|:\\-[./]|:[\\/LS]|=[\\/L]|>.<|:\\$|>:\\-?\\)|>;\\)|\\}:\\-?\\)|3:\\-?\\)|\\(>_<\\)>?|^_?^;|\\(#\\^.\\^#\\)|[Oo]_[Oo]|:\\-?o',s))\n",
    "\n",
    "def getNumberOfElongatedWords(s):\n",
    "    return len(re.findall('([a-zA-Z])\\\\1{2,}', s))\n",
    "    \n",
    "def pronoun(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif PRONOUN.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def exclamation(s):\n",
    "\tc = len(re.findall(r\"[!]\", s))\n",
    "\tif c>=1:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def question(s):\n",
    "\treturn len(re.findall(r\"[?]\", s))\n",
    "\n",
    "def intensifier(sen):\n",
    "\n",
    "\tcount = 0\n",
    "\tfor x in sen:\n",
    "\t\tif INTENSIFIER.__contains__(x)==True:\n",
    "\t\t\tcount+=1\n",
    "\t\t\t#return 1\n",
    "\tif count>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def whword(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif WHWORD.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def religion(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif RELIGION.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def slang(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif SLANG.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def event_phrase(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif EVENT.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def getHashtagopinion(sen):\n",
    "\tfp = codecs.open(OPINION_HASHTAG_PATH,'r','utf-8')\n",
    "\ttemp = set([])\n",
    "\tfor l in fp:\n",
    "\t\ttemp.add(l.strip(' \\t\\n\\r').lower())\n",
    "\tfp.close()\n",
    "\n",
    "\tcur_hash = set([])\n",
    "\tfor x in sen:\n",
    "\t\tif x.startswith('#')==True:\n",
    "\t\t\tcur_hash.add(x.strip(' \\t\\n\\r').lower())\n",
    "\tsize = len(temp.intersection(cur_hash))\n",
    "\tif size>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def numeral(temp):\n",
    "\tc = 0\n",
    "\tfor x in temp:\n",
    "\t\tif x.isdigit()==True:\n",
    "\t\t\tc+=1\n",
    "\treturn c\n",
    "\n",
    "def modal(sen):\n",
    "\tfor x in sen:\n",
    "\t\tif MODAL.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def subjectivity(sen):\n",
    "\t\n",
    "        c = 0\n",
    "        for x in sen:\n",
    "                if SUBJECTIVE.__contains__(x)==True:\n",
    "                        c+=1\n",
    "        tot = len(sen) + 4.0 - 4.0 - 1.0\n",
    "        num = c + 4.0 - 4.0\n",
    "\ttry:\n",
    "        \ts = round(num/tot,4)\n",
    "\texcept:\n",
    "\t\ts = 0\n",
    "\tif c>0:\n",
    "\t\treturn c\n",
    "\treturn 0\n",
    "\t#print(s)\n",
    "\t#return s\n",
    "\n",
    "#HashTagSentimentUnigrams(\"Oh wrestle\", \"NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#HashTagSentimentUnigrams(\"Oh no wrestle\", \"Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#print negatedContextCount(\"this is not for me\")\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "\n",
    "\tREAD_FILES()\n",
    "\ttok = Tokenizer(preserve_case=False)\n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\t#fp = open(TRAIN2,'r')\n",
    "\tfor l in X_train:\n",
    "\t     #wl = l.split('\\t')\n",
    "\t     fo.write(l[0].strip(' \\t\\n\\r') + '\\n')\n",
    "\t#fp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\t#fs = open(TRAIN2,'r')\n",
    "\ts = ''\n",
    "\tN = 0    \n",
    "\thydb_feature = []\n",
    "\thydb_label = []\n",
    "\tfor l,t,u in zip(fp,X_train,y_train):\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\t#row = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(t)\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(t)\n",
    "\t\t\tQ = question(t)\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(t)\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt1 = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\thydb_feature.append(t1)\n",
    "\t\t\thydb_label.append(int(u))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\t#fs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "\n",
    "\thydb_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #hydb_clf = svm.SVC(kernel='rbf')\n",
    "        #hydb_clf = RandomForestClassifier()\n",
    "        #hydb_clf = svm.LinearSVC()\n",
    "        #hydb_clf = LogisticRegression()\n",
    "        #hydb_clf = BernoulliNB()\n",
    "        #X_trainhy,X_testhy,y_trainhy,y_testhy=train_test_split(hydb_feature,hydb_label,test_size=0.25,random_state=1)\n",
    "        hydb_clf.fit(hydb_feature,hydb_label)\n",
    "         \n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\t#fp = open(TRAIN4,'r')\n",
    "\tfor l in X_test:\n",
    "\t     #wl = l.split('\\t')\n",
    "\t     fo.write(l[0].strip(' \\t\\n\\r') + '\\n')\n",
    "\t#fp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\t#fs = open(TRAIN4,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\tutkd_feature = []\n",
    "\tutkd_label = []\n",
    "\tfor l,u,v in zip(fp,X_test,y_test):\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\t#row = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(u)\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(u)\n",
    "\t\t\tQ = question(u)\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(u)\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\tutkd_feature.append(t)\n",
    "\t\t\tutkd_label.append(int(v))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\t#fs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "        \n",
    "\tutkd_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #utkd_clf = svm.SVC(kernel='rbf')\n",
    "        #utkd_clf = RandomForestClassifier()\n",
    "        #utkd_clf = svm.LinearSVC()\n",
    "        #utkd_clf = LogisticRegression()\n",
    "        #utkd_clf = BernoulliNB()\n",
    "        #X_trainu,X_testu,y_trainu,y_testu=train_test_split(utkd_feature,utkd_label,test_size=0.25,random_state=1)\n",
    "        #utkd_clf.fit(utkd_feature,utkd_label)\n",
    "        y_pred_classu=hydb_clf.predict(utkd_feature) \n",
    "        print \"Training Smerp level1 and Testing level2\"\n",
    "        print metrics.classification_report(utkd_label, y_pred_classu)\n",
    "        print metrics.accuracy_score(utkd_label,y_pred_classu)\n",
    "        #print roc_auc_score(utkd_label, y_pred_classu)\n",
    "        print confusion_matrix(utkd_label,y_pred_classu)\n",
    "        #print \"Training Smerp level2 and Testing level1\"\n",
    "        #y_pred_classu2=utkd_clf.predict(hybd_feature) \n",
    "        #print metrics.classification_report(utkd_label, y_pred_classu2)\n",
    "        #print metrics.accuracy_score(utkd_label,y_pred_classu2)\n",
    "        #print roc_auc_score(utkd_label, y_pred_classu2)\n",
    "        #print confusion_matrix(utkd_label,y_pred_classu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
