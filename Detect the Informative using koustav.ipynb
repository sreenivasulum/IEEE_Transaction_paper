{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-c:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "-c:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from scipy import misc\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/new/CrisisMMD_v1.0/annotations/iraq_iran_earthquake_final_data1.tsv\",header=None,names=['tweet_id','image_id','text_info','text_info_conf','image_info','image_info_conf','text_human','text_human_conf','image_human','image_human_conf','image_damage','image_damage_conf','tweet_text','image_url','image_path'],encoding='utf-8')\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(text1P['text_info'])\n",
    "label=le.transform(text1P['text_info']) \n",
    "text1P['label']=label\n",
    "text1P1=text1P.sort_values(by='label')\n",
    "print len(text1P1)\n",
    "textPos=text1P1.head(105)\n",
    "textPos['label1']=1\n",
    "textNeg=text1P1.tail(105)\n",
    "textNeg['label1']=0\n",
    "textTrainP=textPos[:84]\n",
    "textTestP=textPos[84:]\n",
    "textTrainN=textNeg[:84]\n",
    "textTestN=textNeg[84:]\n",
    "framesTrain=[textTrainP,textTrainN]\n",
    "framesTest=[textTestP,textTestN]\n",
    "train=pd.concat(framesTrain)\n",
    "test=pd.concat(framesTest)\n",
    "X_train1=train['tweet_text']\n",
    "y_train1=train['label1']\n",
    "X_test1=test['tweet_text']\n",
    "y_test1=test['label1']\n",
    "#Train1=pd.concat([textPos,textNeg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-c:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/new/CrisisMMD_v1.0/annotations/hurricane_harvey_final_data1.tsv\",header=None,names=['tweet_id','image_id','text_info','text_info_conf','image_info','image_info_conf','text_human','text_human_conf','image_human','image_human_conf','image_damage','image_damage_conf','tweet_text','image_url','image_path'],encoding='utf-8')\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(text1P['text_info'])\n",
    "label=le.transform(text1P['text_info']) \n",
    "text1P['label']=label\n",
    "text1P1=text1P.sort_values(by='label')\n",
    "print len(text1P1)\n",
    "textPos=text1P1.head(1109)\n",
    "textPos['label1']=1\n",
    "textNeg=text1P1.tail(1109)\n",
    "textNeg['label1']=0\n",
    "textTrainP=textPos[:887]\n",
    "textTestP=textPos[887:]\n",
    "textTrainN=textNeg[:887]\n",
    "textTestN=textNeg[887:]\n",
    "framesTrain=[textTrainP,textTrainN]\n",
    "framesTest=[textTestP,textTestN]\n",
    "train=pd.concat(framesTrain)\n",
    "test=pd.concat(framesTest)\n",
    "X_train2=train['tweet_text']\n",
    "y_train2=train['label1']\n",
    "X_test2=test['tweet_text']\n",
    "y_test2=test['label1']\n",
    "#Train2=pd.concat([textPos,textNeg])\n",
    "#print Train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4521\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/new/CrisisMMD_v1.0/annotations/hurricane_irma_final_data1.tsv\",header=None,names=['tweet_id','image_id','text_info','text_info_conf','image_info','image_info_conf','text_human','text_human_conf','image_human','image_human_conf','image_damage','image_damage_conf','tweet_text','image_url','image_path'],encoding='utf-8')\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(text1P['text_info'])\n",
    "label=le.transform(text1P['text_info']) \n",
    "text1P['label']=label\n",
    "text1P1=text1P.sort_values(by='label')\n",
    "print len(text1P1)\n",
    "textPos=text1P1.head(957)\n",
    "textPos['label1']=1\n",
    "textNeg=text1P1.tail(957)\n",
    "textNeg['label1']=0\n",
    "textTrainP=textPos[:766]\n",
    "textTestP=textPos[766:]\n",
    "textTrainN=textNeg[:766]\n",
    "textTestN=textNeg[766:]\n",
    "framesTrain=[textTrainP,textTrainN]\n",
    "framesTest=[textTestP,textTestN]\n",
    "train=pd.concat(framesTrain)\n",
    "test=pd.concat(framesTest)\n",
    "X_train3=train['tweet_text']\n",
    "y_train3=train['label1']\n",
    "X_test3=test['tweet_text']\n",
    "y_test3=test['label1']\n",
    "#Train3=pd.concat([textPos,textNeg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4562\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/new/CrisisMMD_v1.0/annotations/hurricane_maria_final_data1.tsv\",header=None,names=['tweet_id','image_id','text_info','text_info_conf','image_info','image_info_conf','text_human','text_human_conf','image_human','image_human_conf','image_damage','image_damage_conf','tweet_text','image_url','image_path'],encoding='utf-8')\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(text1P['text_info'])\n",
    "label=le.transform(text1P['text_info']) \n",
    "text1P['label']=label\n",
    "text1P1=text1P.sort_values(by='label')\n",
    "print len(text1P1)\n",
    "textPos=text1P1.head(1718)\n",
    "textPos['label1']=1\n",
    "textNeg=text1P1.tail(1718)\n",
    "textNeg['label1']=0\n",
    "textTrainP=textPos[:1374]\n",
    "textTestP=textPos[1374:]\n",
    "textTrainN=textNeg[:1374]\n",
    "textTestN=textNeg[1374:]\n",
    "framesTrain=[textTrainP,textTrainN]\n",
    "framesTest=[textTestP,textTestN]\n",
    "train=pd.concat(framesTrain)\n",
    "test=pd.concat(framesTest)\n",
    "X_train4=train['tweet_text']\n",
    "y_train4=train['label1']\n",
    "X_test4=test['tweet_text']\n",
    "y_test4=test['label1']\n",
    "Train4=pd.concat([textPos,textNeg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNBALANCED RAW TWEETS FOR SITUATIONAL INFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4929\n",
      "1568\n",
      "4982\n",
      "1511\n",
      "4798\n",
      "1871\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "text1=pd.read_table(\"/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/classification/RAW_TRAIN DATA/hagupit_RAW_TWEET.txt\",names=['Date','Time','Tweet_id','Tweets','label'])\n",
    "x_text1=text1['Tweets']\n",
    "labels1=[]\n",
    "c=0\n",
    "\n",
    "for i in text1['label']:\n",
    "    if i==2:\n",
    "        i=0\n",
    "    labels1.append(i)\n",
    "\n",
    "text2=pd.read_table(\"/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/classification/RAW_TRAIN DATA/hydb_RAW_TWEET.txt\",names=['Date','Time','Tweet_id','Tweets','label'])\n",
    "x_text11=text2['Tweets']\n",
    "labels11=[]\n",
    "c=0\n",
    "for i in text2['label']:\n",
    "    if i==2 or i==3:\n",
    "        i=0\n",
    "        c=c+1\n",
    "    labels11.append(i)\n",
    "print len(x_text11)\n",
    "print c\n",
    "text3=pd.read_table(\"/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/classification/RAW_TRAIN DATA/sandy_hook_RAW_TWEET.txt\",names=['Date','Time','Tweet_id','Tweets','label'])\n",
    "x_text111=text3['Tweets']\n",
    "labels111=[]\n",
    "c=0\n",
    "for i in text3['label']:\n",
    "    if i==2:\n",
    "        i=0\n",
    "        c=c+1\n",
    "    labels111.append(i)\n",
    "print len(x_text111)\n",
    "print c\n",
    "text4=pd.read_table(\"/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/classification/RAW_TRAIN DATA/utkd_RAW_TWEET.txt\",names=['Date','Time','Tweet_id','Tweets','label'])\n",
    "x_text1111=text4['Tweets']\n",
    "labels1111=[]\n",
    "c=0\n",
    "for i in text4['label']:\n",
    "    if i==2:\n",
    "        i=0\n",
    "        c=c+1\n",
    "    labels1111.append(i)\n",
    "print len(x_text1111)\n",
    "print c\n",
    "#total=pd.concat([text1,text11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALANCED RAW TWEETS FOR SITUATIONAL INFORMATION DURING DISASTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3128\n",
      "3136\n",
      "3022\n",
      "3742\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "text1=pd.read_table(\"/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/classification/RAW_TRAIN DATA/hagupit_RAW_TWEET.txt\",names=['Date','Time','Tweet_id','Tweets','label'])\n",
    "#x_text1=text1['Tweets']\n",
    "labels1=[]\n",
    "x_text1=[]\n",
    "c=0\n",
    "for i,j in zip(text1['Tweets'],text1['label']):\n",
    "    if c!=1564:\n",
    "        if j==1:\n",
    "            x_text1.append(i)\n",
    "            labels1.append(j)\n",
    "            c=c+1\n",
    "    if j==2:\n",
    "        j=0\n",
    "        x_text1.append(i)\n",
    "        labels1.append(j)\n",
    "print len(x_text1)\n",
    "text2=pd.read_table(\"/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/classification/RAW_TRAIN DATA/hydb_RAW_TWEET.txt\",names=['Date','Time','Tweet_id','Tweets','label'])\n",
    "x_text11=[]\n",
    "labels11=[]\n",
    "c=0\n",
    "for i,j in zip(text2['Tweets'],text2['label']):\n",
    "    if c!=1568:\n",
    "        if j==1:\n",
    "            x_text11.append(i)\n",
    "            labels11.append(j)\n",
    "            c=c+1\n",
    "    if j==2 or j==3:\n",
    "        x_text11.append(i)\n",
    "        labels11.append(j)\n",
    "print len(x_text11)\n",
    "text3=pd.read_table(\"/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/classification/RAW_TRAIN DATA/sandy_hook_RAW_TWEET.txt\",names=['Date','Time','Tweet_id','Tweets','label'])\n",
    "x_text111=[]\n",
    "labels111=[]\n",
    "c=0\n",
    "for i,j in zip(text3['Tweets'],text3['label']):\n",
    "    if c!=1511:\n",
    "        if j==1:\n",
    "            x_text111.append(i)\n",
    "            labels111.append(j)\n",
    "            c=c+1\n",
    "    if j==2:\n",
    "        x_text111.append(i)\n",
    "        labels111.append(j)\n",
    "print len(x_text111)\n",
    "text4=pd.read_table(\"/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/classification/RAW_TRAIN DATA/utkd_RAW_TWEET.txt\",names=['Date','Time','Tweet_id','Tweets','label'])\n",
    "x_text1111=[]\n",
    "labels1111=[]\n",
    "c=0\n",
    "for i,j in zip(text4['Tweets'],text4['label']):\n",
    "    if c!=1871:\n",
    "        if j==1:\n",
    "            x_text1111.append(i)\n",
    "            labels1111.append(j)\n",
    "            c=c+1\n",
    "    if j==2:\n",
    "        x_text1111.append(i)\n",
    "        labels1111.append(j)\n",
    "print len(x_text1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRAGMENT TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text1=pd.read_table(\"/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/classification/fragmented_train_data/hagupit_fragment_train.txt\",names=['Tweets','label'])\n",
    "x_text=text1['Tweets']\n",
    "labels=[]\n",
    "for i in text1['label']:\n",
    "    if i==2:\n",
    "        i=0\n",
    "    labels.append(i)\n",
    "text2=pd.read_table(\"/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/classification/fragmented_train_data/hydb_fragment_train.txt\",names=['Tweets','label'])\n",
    "x_text11=text2['Tweets']\n",
    "labels11=[]\n",
    "for i in text2['label']:\n",
    "    if i==2:\n",
    "        i=0\n",
    "    labels11.append(i)\n",
    "text3=pd.read_table(\"/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/classification/fragmented_train_data/sandy_hook_fragment_train.txt\",names=['Tweets','label'])\n",
    "x_text111=text3['Tweets']\n",
    "labels111=[]\n",
    "for i in text3['label']:\n",
    "    if i==2:\n",
    "        i=0\n",
    "    labels111.append(i)\n",
    "text4=pd.read_table(\"/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/classification/fragmented_train_data/utkd_fragment_train.txt\",names=['Tweets','label'])\n",
    "x_text1111=text4['Tweets']\n",
    "labels1111=[]\n",
    "for i in text4['label']:\n",
    "    if i==2:\n",
    "        i=0\n",
    "    labels1111.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need and Availability  for fire 2017 using Koustav rudra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(664, 2)\n",
      "(1226,)\n",
      "(307,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sreenu/.local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/FIRE/Fire2017-IRMiDis-data/microblogs-crawl-directory/Availabilty1.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1P['label']=1\n",
    "print text1P.shape\n",
    "#print len(text1P)\n",
    "text1P1=pd.read_table(\"/home/sreenu/Desktop/dataset4may/FIRE/Fire2017-IRMiDis-data/microblogs-crawl-directory/Need1.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1P1['label']=2\n",
    "#print text1P1.shape\n",
    "text1N=pd.read_table(\"/home/sreenu/Desktop/dataset4may/FIRE/FIRE2016-microblogs-track-data/nonrelevantresource\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1N['label']=0\n",
    "textTrainP=text1P[:531]\n",
    "textTestP=text1P[531:]\n",
    "textTrainP1=text1P1[:164]\n",
    "textTestP1=text1P1[164:]\n",
    "textTrainN=text1N[:531]\n",
    "textTestN=text1N[531:]\n",
    "framesTrain=[textTrainP,textTrainP1,textTrainN]\n",
    "framesTest=[textTestP,textTestP1,textTestN]\n",
    "train=pd.concat(framesTrain)\n",
    "test=pd.concat(framesTest)\n",
    "X_train=train['tweet1']\n",
    "y_train=train['label']\n",
    "X_test=test['tweet1']\n",
    "y_test=test['label']\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need and Availability  for Nepal Earthquake using Koustav rudra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/Paperswithcode/applyandcompareneuralmodels/data/nepal/Availabilitynepal.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1P['label']=1\n",
    "print text1P.shape\n",
    "#print len(text1P)\n",
    "text1P1=pd.read_table(\"/home/sreenu/Desktop/dataset4may/Paperswithcode/applyandcompareneuralmodels/data/nepal/neednepal.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1P1['label']=2\n",
    "#print text1P1.shape\n",
    "text1N=pd.read_table(\"/home/sreenu/Desktop/dataset4may/FIRE/FIRE2016-microblogs-track-data/Nonrelevantresource.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1N['label']=0\n",
    "textTrainP=text1P[:944]\n",
    "textTestP=text1P[944:]\n",
    "textTrainP1=text1P1[:372]\n",
    "textTestP1=text1P1[372:]\n",
    "textTrainN=text1N[:944]\n",
    "textTestN=text1N[944:]\n",
    "framesTrain=[textTrainP,textTrainP1,textTrainN]\n",
    "framesTest=[textTestP,textTestP1,textTestN]\n",
    "train=pd.concat(framesTrain)\n",
    "test=pd.concat(framesTest)\n",
    "X_train=train['tweet1']\n",
    "y_train=train['label']\n",
    "X_test=test['tweet1']\n",
    "y_test=test['label']\n",
    "print textTrainP.shape\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure damage for SMERP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import cross_validation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "text1=pd.read_table(\"/home/sreenu/Desktop/dataset4may/SMERP/RetrievedSMERP/level-1/TotalSMERP_T31.tsv\",header=None,names=['tweet1','label','label1'],encoding='utf-8')\n",
    "text2=text1.dropna(thresh=0.8*len(text1), axis=1)\n",
    "textTrainP=text2[:878]\n",
    "textTestP=text2[878:1170]\n",
    "textTrainN=text2[1170:2048]\n",
    "textTestN=text2[2048:]\n",
    "framesTrain=[textTrainP,textTrainN]\n",
    "framesTest=[textTestP,textTestN]\n",
    "#X_train=pd.concat(framesTrain)['tweet1']\n",
    "#y_train=pd.concat(framesTrain)['label']\n",
    "#X_test=pd.concat(framesTest)['tweet1']\n",
    "#y_test=pd.concat(framesTest)['label']\n",
    "#print X_train.shape\n",
    "#X_train,X_test,y_train,y_test=train_test_split(text1.tweet1,text1.label,test_size=0.25,random_state=1)\n",
    "#X_train,X_test,y_train,y_test=train_test_split(X_train11,y_train11,test_size=0.25,random_state=1)\n",
    "X_test=text1['tweet1']\n",
    "y_test=text1['label']\n",
    "#X_test=text2['tweet1']\n",
    "#y_test=text2['label']\n",
    "#print len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure damage for SMERP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:8: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import sys,os\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/SMERP/RetrievedSMERP/level-2/SMERP_T3_level2.tsv\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1P['label']=1\n",
    "text1N=pd.read_table(\"/home/sreenu/Desktop/dataset4may/SMERP/RetrievedSMERP/level-2/NONSMERP-level2-T3.tsv\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1N['label']=0\n",
    "totalframes=[text1P,text1N]\n",
    "\n",
    "trainP=text1P[:636]\n",
    "testP=text1P[636:]\n",
    "trainN=text1N[:636]\n",
    "testN=text1N[636:]\n",
    "frames=[trainP,trainN]\n",
    "frames1=[testP,testN]\n",
    "framesTrain=pd.concat(frames)\n",
    "framesTest=pd.concat(frames1)\n",
    "#X_test=pd.concat(totalframes)['tweet1']\n",
    "#y_test=pd.concat(totalframes)['label']\n",
    "#X_test=pd.concat(frames1)['tweet1']\n",
    "#y_test=pd.concat(frames1)['label']\n",
    "#train=[textTrainP,trainP,textTrainN,trainN]\n",
    "#X_train=pd.concat(frames)['tweet1']\n",
    "#y_train=pd.concat(frames)['label']\n",
    "#test=[textTestP,testP,textTestN,testN]\n",
    "#X_test=pd.concat(frames1)['tweet1']\n",
    "#y_test=pd.concat(frames1)['label']\n",
    "#print len(X_train)\n",
    "#print len(X_test)\n",
    "#print len(y_train)\n",
    "#print len(y_test)\n",
    "X_train=pd.concat(totalframes)['tweet1']\n",
    "y_train=pd.concat(totalframes)['label']\n",
    "#X_train,X_test,y_train,y_test=train_test_split(X_train11,y_train11,test_size=0.25,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOTH SMERP1 and SMERP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:8: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "text1=pd.read_table(\"/home/sreenu/Desktop/dataset4may/SMERP/RetrievedSMERP/level-1/TotalSMERP_T31.tsv\",header=None,names=['tweet1','label','label1'],encoding='utf-8')\n",
    "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/SMERP/RetrievedSMERP/level-2/SMERP_T3_level2.tsv\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1P['label']=1\n",
    "text1N=pd.read_table(\"/home/sreenu/Desktop/dataset4may/SMERP/RetrievedSMERP/level-2/NONSMERP-level2-T3.tsv\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1N['label']=0\n",
    "Allframes=[text1,text1P,text1N]\n",
    "X_train=pd.concat(Allframes)['tweet1']\n",
    "y_train=pd.concat(Allframes)['label']\n",
    "print len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure damage for FIRE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P=pd.read_table(\"/home/sreenu/Desktop/dataset4may/FIRE/FIRE2016-microblogs-track-data/RetrievedFIRE/FMT7tweets.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1P['label']=1\n",
    "text1N=pd.read_table(\"/home/sreenu/Desktop/dataset4may/FIRE/FIRE2016-microblogs-track-data/RetrievedFIRE/Nonrelevant.txt\",header=None,names=['tweet1'],encoding='utf-8')\n",
    "text1N['label']=0\n",
    "text1N=text1N[:241]\n",
    "totalframes=[text1P,text1N]\n",
    "trainP=text1P[:182]\n",
    "testP=text1P[182:]\n",
    "trainN=text1N[:182]\n",
    "testN=text1N[182:241]\n",
    "frames=[trainP,trainN]\n",
    "frames1=[testP,testN]\n",
    "framesTrain=pd.concat(frames)\n",
    "framesTest=pd.concat(frames1)\n",
    "#X_train=pd.concat(frames)['tweet1']\n",
    "#y_train=pd.concat(frames)['label']\n",
    "#X_test=pd.concat(frames1)['tweet1']\n",
    "#y_test=pd.concat(frames1)['label']\n",
    "X_test=pd.concat(totalframes)['tweet1']\n",
    "y_test=pd.concat(totalframes)['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure damage for NEPAL EARTHQUAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:8: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:13: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:13: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "text1P1=pd.read_table(\"/home/sreenu/Desktop/TweetCrawler/CrisisNLP_volunteers_labels.DS_Store/2015_Nepal_Earthquake_en/Filtered tweets - 2015_Nepal_Earthquake_en.csv/infrastructuredamage/Infrastructureandutilities.csv\",header=None,sep=\",u\",names=['tweetid','tweet1'],encoding='utf-8')\n",
    "text1P1['label']=1\n",
    "text1P2=pd.read_table(\"/home/sreenu/Desktop/TweetCrawler/CrisisNLP_volunteers_labels.DS_Store/2015_Nepal_Earthquake_en/Filtered tweets - 2015_Nepal_Earthquake_en.csv/infrastructuredamage/Infrastructuredamage.csv\",header=None,sep=\",u\",names=['tweetid','tweet1'],encoding='utf-8')\n",
    "text1P2['label']=1\n",
    "text1P3=pd.read_table(\"/home/sreenu/Desktop/TweetCrawler/CrisisNLP_volunteers_labels.DS_Store/2015_Nepal_Earthquake_en/Filtered tweets - 2015_Nepal_Earthquake_en.csv/infrastructuredamage/Injuredordeadpeople.csv\",header=None,sep=\",u\",names=['tweetid','tweet1'],encoding='utf-8')\n",
    "text1P3['label']=1\n",
    "text1P4=pd.read_table(\"/home/sreenu/Desktop/TweetCrawler/CrisisNLP_volunteers_labels.DS_Store/2015_Nepal_Earthquake_en/Filtered tweets - 2015_Nepal_Earthquake_en.csv/infrastructuredamage/Missingtrappedorfoundpeople.csv\",header=None,sep=\",u\",names=['tweetid','tweet1'],encoding='utf-8')\n",
    "text1P4['label']=1\n",
    "#print text1P.shape\n",
    "text1N=pd.read_table(\"/home/sreenu/Desktop/TweetCrawler/CrisisNLP_volunteers_labels.DS_Store/2015_Nepal_Earthquake_en/Filtered tweets - 2015_Nepal_Earthquake_en.csv/infrastructuredamage/Otherrelevantinformation.csv\",header=None,sep=\",u\",names=['tweetid','tweet1'],encoding='utf-8')\n",
    "text1N['label']=0\n",
    "print len(text1N)\n",
    "frames=[text1P1,text1P2,text1P3,text1P4,text1N]\n",
    "result3=pd.concat(frames)\n",
    "trainP=result3[:227]\n",
    "testP=result3[227:302]\n",
    "trainN=text1N[:227]\n",
    "testN=text1N[227:]\n",
    "frames1=[trainP,trainN]\n",
    "frames2=[testP,testN]\n",
    "framesTrain=pd.concat(frames1)\n",
    "framesTest=pd.concat(frames2)\n",
    "#X_train=pd.concat(frames1)['tweet1']\n",
    "#y_train=pd.concat(frames1)['label']\n",
    "#X_test=pd.concat(frames2)['tweet1']\n",
    "#y_test=pd.concat(frames2)['label']\n",
    "X_test=pd.concat(frames)['tweet1']\n",
    "y_test=pd.concat(frames)['label']\n",
    "#print result3.shape\n",
    "#print len(result3)\n",
    "#print text1N.shape\n",
    "#print result.label\n",
    "#text2=text1.dropna(thresh=0.8*len(text1), axis=1)\n",
    "#X_test=result3.tweet1\n",
    "#y_test=result3.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K_FOLD CROSS_VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hydb: ', 0.59875, 0.08510716283995529)\n",
      "('Recall', 0.6375, array([0.5  , 0.875, 0.625, 0.625, 0.75 , 0.5  , 0.5  , 0.5  , 0.875,\n",
      "       0.625]))\n",
      "('Precision', 0.5985137085137084, array([0.57142857, 0.7       , 0.55555556, 0.71428571, 0.54545455,\n",
      "       0.57142857, 0.5       , 0.57142857, 0.7       , 0.55555556]))\n",
      "('F1', 0.6130271757825937, array([0.53333333, 0.77777778, 0.58823529, 0.66666667, 0.63157895,\n",
      "       0.53333333, 0.5       , 0.53333333, 0.77777778, 0.58823529]))\n",
      "('Utkd: ', 0.6645338837289303, 0.16468402903121354)\n",
      "('Recall', 0.7244444444444444, array([1.        , 0.9       , 0.9       , 0.88888889, 0.88888889,\n",
      "       1.        , 0.22222222, 0.55555556, 0.44444444, 0.44444444]))\n",
      "('Precision', 0.7119047619047618, array([0.66666667, 1.        , 0.6       , 0.8       , 0.53333333,\n",
      "       0.6       , 0.28571429, 0.83333333, 0.8       , 1.        ]))\n",
      "('F1', 0.6829620204357045, array([0.8       , 0.94736842, 0.72      , 0.84210526, 0.66666667,\n",
      "       0.75      , 0.25      , 0.66666667, 0.57142857, 0.61538462]))\n",
      "('SandyHook: ', 0.6668498168498167, 0.07039931876401723)\n",
      "('Recall', 0.85, array([1.        , 1.        , 0.85714286, 0.71428571, 0.71428571,\n",
      "       0.71428571, 1.        , 0.83333333, 0.66666667, 1.        ]))\n",
      "('Precision', 0.6108519258519258, array([0.53846154, 0.58333333, 0.6       , 0.625     , 0.71428571,\n",
      "       0.625     , 0.54545455, 0.55555556, 0.57142857, 0.75      ]))\n",
      "('F1', 0.7035419997958697, array([0.7       , 0.73684211, 0.70588235, 0.66666667, 0.71428571,\n",
      "       0.66666667, 0.70588235, 0.66666667, 0.61538462, 0.85714286]))\n",
      "('Hagupit: ', 0.6557359307359307, 0.22548466381205953)\n",
      "('Recall', 0.6, array([1.        , 1.        , 0.81818182, 1.        , 0.90909091,\n",
      "       0.63636364, 0.        , 0.09090909, 0.18181818, 0.36363636]))\n",
      "('Precision', 0.6848040848040847, array([0.57142857, 0.92307692, 0.75      , 0.91666667, 0.90909091,\n",
      "       0.77777778, 0.        , 1.        , 0.33333333, 0.66666667]))\n",
      "('F1', 0.5908043090754089, array([0.72727273, 0.96      , 0.7826087 , 0.95652174, 0.90909091,\n",
      "       0.7       , 0.        , 0.16666667, 0.23529412, 0.47058824]))\n",
      "Train Hydb\n",
      "('Utkd: ', 0.39655172413793105)\n",
      "('Sandy_Hook: ', 0.41134751773049644)\n",
      "('Hagupit: ', 0.2682926829268293)\n",
      "Train Utkd\n",
      "('Hydb: ', 0.2611464968152866)\n",
      "('Sandy_Hook: ', 0.6595744680851063)\n",
      "('Hagupit: ', 0.5853658536585366)\n",
      "Train Sandy_Hook\n",
      "('Hydb: ', 0.24203821656050956)\n",
      "('Utkd: ', 0.5229885057471264)\n",
      "('Hagupit: ', 0.5756097560975609)\n",
      "Train Hagupit\n",
      "('Hydb: ', 0.2802547770700637)\n",
      "('Utkd: ', 0.5804597701149425)\n",
      "('Sandy_Hook: ', 0.6028368794326241)\n",
      "\n",
      "\n",
      "\n",
      "Hydb -> utkd | sandy_hook | hagupit\n",
      "('PRECISION: ', 0.6764705882352942)\n",
      "('RECALL: ', 0.7419354838709677)\n",
      "('F1_SCORE: ', 0.7076923076923077)\n",
      "('PRECISION: ', 0.58)\n",
      "('RECALL: ', 0.8787878787878788)\n",
      "('F1_SCORE: ', 0.6987951807228915)\n",
      "('PRECISION: ', 0.7333333333333333)\n",
      "('RECALL: ', 0.49107142857142855)\n",
      "('F1_SCORE: ', 0.5882352941176471)\n",
      "\n",
      "\n",
      "\n",
      "Utkd -> hydb | sandy_hook | hagupit\n",
      "('PRECISION: ', 0.6212121212121212)\n",
      "('RECALL: ', 0.5125)\n",
      "('F1_SCORE: ', 0.5616438356164383)\n",
      "('PRECISION: ', 0.59)\n",
      "('RECALL: ', 0.8939393939393939)\n",
      "('F1_SCORE: ', 0.7108433734939759)\n",
      "('PRECISION: ', 0.6483516483516484)\n",
      "('RECALL: ', 0.5267857142857143)\n",
      "('F1_SCORE: ', 0.58128078817734)\n",
      "\n",
      "\n",
      "\n",
      "Sandy_hook -> hydb | utkd | hagupit\n",
      "('PRECISION: ', 0.6440677966101694)\n",
      "('RECALL: ', 0.475)\n",
      "('F1_SCORE: ', 0.5467625899280575)\n",
      "('PRECISION: ', 0.6388888888888888)\n",
      "('RECALL: ', 0.24731182795698925)\n",
      "('F1_SCORE: ', 0.35658914728682173)\n",
      "('PRECISION: ', 0.8048780487804879)\n",
      "('RECALL: ', 0.29464285714285715)\n",
      "('F1_SCORE: ', 0.4313725490196078)\n",
      "\n",
      "\n",
      "\n",
      "Hagupit -> hydb | utkd | sandy_hook\n",
      "('PRECISION: ', 0.5714285714285714)\n",
      "('RECALL: ', 0.55)\n",
      "('F1_SCORE: ', 0.5605095541401274)\n",
      "('PRECISION: ', 0.5632911392405063)\n",
      "('RECALL: ', 0.956989247311828)\n",
      "('F1_SCORE: ', 0.7091633466135459)\n",
      "('PRECISION: ', 0.5471698113207547)\n",
      "('RECALL: ', 0.8787878787878788)\n",
      "('F1_SCORE: ', 0.6744186046511628)\n",
      "(4, 12, 12, 12, 12)\n",
      "('Average Indomain Accuracy: ', 0.6464674078286694)\n",
      "('Average Cross-domain Accuracy: ', 0.44887222069808447)\n",
      "('Average and std Precision: ', 0.6349243289501479, 0.07226172723163782)\n",
      "('Average and std Recall: ', 0.6206459758879114, 0.23183266327967555)\n",
      "('Average and std F-score: ', 0.5939422142883269, 0.10980914055650552)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on 11-May-2015\n",
    "\n",
    "@author: Koustav\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "from textblob import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn import svm\n",
    "from sklearn.svm import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "import gzip\n",
    "from happyfuntokenizing import *\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import *\n",
    "#from happyfuntokenizing import *\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "mycompile = lambda pat:  re.compile(pat,  re.UNICODE)\n",
    "\n",
    "PRONOUN_PATH = '../../Lexical_resources/classifier_dictionary/english_pronoun.txt'\n",
    "WHWORD_PATH = '../classifier_dictionary/english_whwords.txt'\n",
    "SLANG_PATH = '../classifier_dictionary/english_swear.txt'\n",
    "INTENSIFIER_PATH = '../classifier_dictionary/english_intensifier.txt'\n",
    "SUBJECTIVE_PATH = '../classifier_dictionary/subjclueslen1-HLTEMNLP05.tff'\n",
    "EVENT_PATH = '../classifier_dictionary/english_nonsituational_phrase.txt'\n",
    "MODAL_VERB_PATH = '../classifier_dictionary/english_modal_verb.txt'\n",
    "RELIGION_PATH = '../classifier_dictionary/communal_race.txt'\n",
    "#NONSIT_PATH = 'Common_nonsituational_word.txt'\n",
    "#OPINION_HASHTAG_PATH = '/home/krudra/twitter_code/shared/language_overlap/code_mix_pitch/devanagari/devanagari_hashtag_opinion.txt'\n",
    "#MENTION_PATH = '/home/krudra/twitter_code/shared/language_overlap/code_mix_pitch/devanagari/news_mention.txt'\n",
    "\n",
    "#TRAIN1 = '../classification_4.3/fragmented_train_data/hydb_fragment_train.txt'\n",
    "#TRAIN2 = '../classification_4.3/fragmented_train_data/utkd_fragment_train.txt'\n",
    "#TRAIN3 = '../classification_4.3/fragmented_train_data/sandy_hook_fragment_train.txt'\n",
    "#TRAIN4 = '../classification_4.3/fragmented_train_data/hagupit_fragment_train.txt'\n",
    "#TRAIN1=\"../dataset4may/SMERP/RetrievedSMERP/level-1/TotalSMERP_T31.tsv\"\n",
    "\n",
    "'''TRAIN1 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hydb_RAW_CLASS.txt'\n",
    "TRAIN2 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/utkd_RAW_CLASS.txt'\n",
    "TRAIN3 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/sandy_hook_RAW_CLASS.txt'\n",
    "TRAIN4 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hagupit_RAW_CLASS.txt' '''\n",
    "\n",
    "'''TRAIN1 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hydb_balance_RAW.txt'\n",
    "TRAIN2 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/utkd_balance_RAW.txt'\n",
    "TRAIN3 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/sandy_hook_balance_RAW.txt'\n",
    "TRAIN4 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hagupit_balance_RAW.txt' '''\n",
    "\n",
    "\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "TAGGER_PATH = '../../Lexical_resources/ark-tweet-nlp-0.3.2/'\n",
    "\n",
    "PRONOUN = {}\n",
    "WHWORD = {}\n",
    "SLANG = {}\n",
    "INTENSIFIER = {}\n",
    "SUBJECTIVE = {}\n",
    "EVENT = {}\n",
    "MODAL = {}\n",
    "RELIGION = {}\n",
    "\n",
    "def READ_FILES():\n",
    "\n",
    "\tfp = open(PRONOUN_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tPRONOUN[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(INTENSIFIER_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tINTENSIFIER[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(WHWORD_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tWHWORD[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(SLANG_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tSLANG[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\tfp = open(EVENT_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tEVENT[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\t'''fp = open(NONSIT_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tEVENT[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close() '''\n",
    "\t\n",
    "\tfp = open(MODAL_VERB_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tMODAL[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(RELIGION_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tRELIGION[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\tfp = open(SUBJECTIVE_PATH,'r')\n",
    "        for l in fp:\n",
    "                wl = l.split()\n",
    "                x = wl[0].split('=')[1].strip(' \\t\\n\\r')\n",
    "                if x=='strongsubj':\n",
    "                        y = wl[2].split('=')[1].strip(' \\t\\n\\r')\n",
    "                        SUBJECTIVE[y.lower()] = 1\n",
    "\tfp.close()\n",
    "\n",
    "############################ This Functions are used #############################################\n",
    "\n",
    "def emoticons(s):\n",
    "        return len(re.findall(u'[\\U0001f600-\\U0001f60f\\U0001f617-\\U0001f61d\\U0001f632\\U0001f633\\U0001f638-\\U0001f63e\\U0001f642\\U0001f646-\\U0001f64f\\U0001f612\\U0001f613\\U0001f615\\U0001f616\\U0001f61e-\\U0001f629\\U0001f62c\\U0001f62d\\U0001f630\\U0001f631\\U0001f636\\U0001f637\\U0001f63c\\U0001f63f-\\U0001f641\\U0001f64d]', s))\n",
    "\n",
    "def smileys(s):\n",
    "        return len(re.findall(r':\\-\\)|:[\\)\\]\\}]|:[dDpP]|:3|:c\\)|:>|=\\]|8\\)|=\\)|:\\^\\)|:\\-D|[xX8]\\-?D|=\\-?D|=\\-?3|B\\^D|:\\'\\-?\\)|>:\\[|:\\-?\\(|:\\-?c|:\\-?<|:\\-?\\[|:\\{|;\\(|:\\-\\|\\||:@|>:\\(|:\\'\\-?\\(|D:<?|D[8;=X]|v.v|D\\-\\':|>:[\\/]|:\\-[./]|:[\\/LS]|=[\\/L]|>.<|:\\$|>:\\-?\\)|>;\\)|\\}:\\-?\\)|3:\\-?\\)|\\(>_<\\)>?|^_?^;|\\(#\\^.\\^#\\)|[Oo]_[Oo]|:\\-?o',s))\n",
    "\n",
    "def getNumberOfElongatedWords(s):\n",
    "    return len(re.findall('([a-zA-Z])\\\\1{2,}', s))\n",
    "    \n",
    "def pronoun(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif PRONOUN.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def exclamation(s):\n",
    "\tc = len(re.findall(r\"[!]\", s))\n",
    "\tif c>=1:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def question(s):\n",
    "\treturn len(re.findall(r\"[?]\", s))\n",
    "\n",
    "def intensifier(sen):\n",
    "\n",
    "\tcount = 0\n",
    "\tfor x in sen:\n",
    "\t\tif INTENSIFIER.__contains__(x)==True:\n",
    "\t\t\tcount+=1\n",
    "\t\t\t#return 1\n",
    "\tif count>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def whword(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif WHWORD.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def religion(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif RELIGION.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def slang(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif SLANG.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def event_phrase(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif EVENT.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def getHashtagopinion(sen):\n",
    "\tfp = codecs.open(OPINION_HASHTAG_PATH,'r','utf-8')\n",
    "\ttemp = set([])\n",
    "\tfor l in fp:\n",
    "\t\ttemp.add(l.strip(' \\t\\n\\r').lower())\n",
    "\tfp.close()\n",
    "\n",
    "\tcur_hash = set([])\n",
    "\tfor x in sen:\n",
    "\t\tif x.startswith('#')==True:\n",
    "\t\t\tcur_hash.add(x.strip(' \\t\\n\\r').lower())\n",
    "\tsize = len(temp.intersection(cur_hash))\n",
    "\tif size>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def numeral(temp):\n",
    "\tc = 0\n",
    "\tfor x in temp:\n",
    "\t\tif x.isdigit()==True:\n",
    "\t\t\tc+=1\n",
    "\treturn c\n",
    "\n",
    "def modal(sen):\n",
    "\tfor x in sen:\n",
    "\t\tif MODAL.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def subjectivity(sen):\n",
    "\t\n",
    "        c = 0\n",
    "        for x in sen:\n",
    "                if SUBJECTIVE.__contains__(x)==True:\n",
    "                        c+=1\n",
    "        tot = len(sen) + 4.0 - 4.0 - 1.0\n",
    "        num = c + 4.0 - 4.0\n",
    "\ttry:\n",
    "        \ts = round(num/tot,4)\n",
    "\texcept:\n",
    "\t\ts = 0\n",
    "\tif c>0:\n",
    "\t\treturn c\n",
    "\treturn 0\n",
    "\t#print(s)\n",
    "\t#return s\n",
    "\n",
    "#HashTagSentimentUnigrams(\"Oh wrestle\", \"NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#HashTagSentimentUnigrams(\"Oh no wrestle\", \"Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#print negatedContextCount(\"this is not for me\")\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "\n",
    "\tREAD_FILES()\n",
    "\ttok = Tokenizer(preserve_case=False)\n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\t#fp = open(TRAIN1,'r')\n",
    "\tfor l in X_train:\n",
    "\t     #wl = l.split('\\t')\n",
    "\t     fo.write(l.strip(' \\t\\n\\r') + '\\n')\n",
    "\t#fp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\t#fs = open(TRAIN1,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\thydb_feature = []\n",
    "\thydb_label = []\n",
    "\tfor l,k,j in zip(fp,X_train,):\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\t#row = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(i)\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(k)\n",
    "\t\t\tQ = question(k)\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(k)\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\thydb_feature.append(t)\n",
    "\t\t\thydb_label.append(int(j))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\t#fs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "\n",
    "        hydb_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #hydb_clf = svm.SVC(kernel='rbf')\n",
    "        #hydb_clf = RandomForestClassifier()\n",
    "        #hydb_clf = svm.LinearSVC()\n",
    "        #hydb_clf = LogisticRegression()\n",
    "        #hydb_clf = BernoulliNB()\n",
    "        hydb_clf.fit(hydb_feature,hydb_label)\n",
    "        scores = cross_validation.cross_val_score(hydb_clf,hydb_feature,hydb_label,cv=10)\n",
    "        print('Hydb: ',scores.mean(),scores.std())\n",
    "\trecall = cross_validation.cross_val_score(hydb_clf, hydb_feature, hydb_label, cv=10, scoring='recall')\n",
    "        print('Recall', np.mean(recall), recall)\n",
    "        precision = cross_validation.cross_val_score(hydb_clf, hydb_feature, hydb_label, cv=10, scoring='precision')\n",
    "        print('Precision', np.mean(precision), precision)\n",
    "        f1 = cross_validation.cross_val_score(hydb_clf, hydb_feature, hydb_label, cv=10, scoring='f1')\n",
    "        print('F1', np.mean(f1), f1)\n",
    "\tIN.append(scores.mean())\n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\t#fp = open(TRAIN2,'r')\n",
    "\tfor l in x_text11:\n",
    "\t\t#wl = l.split('\\t')\n",
    "\t\tfo.write(l.strip(' \\t\\n\\r') + '\\n')\n",
    "\t#fp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\t#fs = open(TRAIN2,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\tutkd_feature = []\n",
    "\tutkd_label = []\n",
    "\tfor l,k,j in zip(fp,x_text11,labels11):\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\t#row = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(i)\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\tE = exclamation(k)\n",
    "\t\t\tQ = question(k)\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(k)\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\tutkd_feature.append(t)\n",
    "\t\t\tutkd_label.append(int(j))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\t#fs.close()\n",
    "        \n",
    "\tutkd_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #utkd_clf = svm.SVC(kernel='rbf')\n",
    "        #utkd_clf = RandomForestClassifier()\n",
    "        #utkd_clf = svm.LinearSVC()\n",
    "        #utkd_clf = LogisticRegression()\n",
    "        #utkd_clf = BernoulliNB()\n",
    "        utkd_clf.fit(utkd_feature,utkd_label)\n",
    "        scores = cross_validation.cross_val_score(utkd_clf,utkd_feature,utkd_label,cv=10)\n",
    "        print('Utkd: ',scores.mean(),scores.std())\n",
    "\trecall = cross_validation.cross_val_score(utkd_clf, utkd_feature, utkd_label, cv=10, scoring='recall')\n",
    "        print('Recall', np.mean(recall), recall)\n",
    "        precision = cross_validation.cross_val_score(utkd_clf, utkd_feature, utkd_label, cv=10, scoring='precision')\n",
    "        print('Precision', np.mean(precision), precision)\n",
    "        f1 = cross_validation.cross_val_score(utkd_clf, utkd_feature, utkd_label, cv=10, scoring='f1')\n",
    "        print('F1', np.mean(f1), f1)\n",
    "\tIN.append(scores.mean())\n",
    "\t\n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\t#fp = open(TRAIN3,'r')\n",
    "\tfor l in x_text111:\n",
    "\t\t#wl = l.split('\\t')\n",
    "\t\tfo.write(l.strip(' \\t\\n\\r') + '\\n')\n",
    "\t#fp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\t#fs = open(TRAIN3,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\tsandy_hook_feature = []\n",
    "\tsandy_hook_label = []\n",
    "\tfor l,k,j in zip(fp,x_text111,labels111):\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\t#row = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(i)\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\tE = exclamation(k)\n",
    "\t\t\tQ = question(k)\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(k)\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\tsandy_hook_feature.append(t)\n",
    "\t\t\tsandy_hook_label.append(int(j))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\t#fs.close()\n",
    "\n",
    "        sandy_hook_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #sandy_hook_clf = svm.SVC(kernel='rbf')\n",
    "        #sandy_hook_clf = RandomForestClassifier()\n",
    "        #sandy_hook_clf = svm.LinearSVC()\n",
    "        #sandy_hook_clf = LogisticRegression()\n",
    "        #sandy_hook_clf = BernoulliNB()\n",
    "        sandy_hook_clf.fit(sandy_hook_feature,sandy_hook_label)\n",
    "        scores = cross_validation.cross_val_score(sandy_hook_clf,sandy_hook_feature,sandy_hook_label,cv=10)\n",
    "        print('SandyHook: ',scores.mean(),scores.std())\n",
    "\trecall = cross_validation.cross_val_score(sandy_hook_clf, sandy_hook_feature, sandy_hook_label, cv=10, scoring='recall')\n",
    "        print('Recall', np.mean(recall), recall)\n",
    "        precision = cross_validation.cross_val_score(sandy_hook_clf, sandy_hook_feature, sandy_hook_label, cv=10, scoring='precision')\n",
    "        print('Precision', np.mean(precision), precision)\n",
    "        f1 = cross_validation.cross_val_score(sandy_hook_clf, sandy_hook_feature, sandy_hook_label, cv=10, scoring='f1')\n",
    "        print('F1', np.mean(f1), f1)\n",
    "\tIN.append(scores.mean())\n",
    "\t\n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\t#fp = open(TRAIN4,'r')\n",
    "\tfor l in x_text1111:\n",
    "\t\t#wl = l.split('\\t')\n",
    "\t\tfo.write(l.strip(' \\t\\n\\r') + '\\n')\n",
    "\tfp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\t#fs = open(TRAIN4,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\thagupit_feature = []\n",
    "\thagupit_label = []\n",
    "\tfor l,i,j in zip(fp,x_text1111,labels1111):\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\t#row = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(i)\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\tE = exclamation(k)\n",
    "\t\t\tQ = question(k)\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(k)\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\thagupit_feature.append(t)\n",
    "\t\t\thagupit_label.append(int(j))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\t#fs.close()\n",
    "\n",
    "        hagupit_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #hagupit_clf = svm.SVC(kernel='rbf')\n",
    "        #hagupit_clf = RandomForestClassifier()\n",
    "        #hagupit_clf = svm.LinearSVC()\n",
    "        #hagupit_clf = LogisticRegression()\n",
    "        #hagupit_clf = BernoulliNB()\n",
    "        hagupit_clf.fit(hagupit_feature,hagupit_label)\n",
    "        scores = cross_validation.cross_val_score(hagupit_clf,hagupit_feature,hagupit_label,cv=10)\n",
    "        print('Hagupit: ',scores.mean(),scores.std())\n",
    "\trecall = cross_validation.cross_val_score(hagupit_clf, hagupit_feature, hagupit_label, cv=10, scoring='recall')\n",
    "        print('Recall', np.mean(recall), recall)\n",
    "        precision = cross_validation.cross_val_score(hagupit_clf, hagupit_feature, hagupit_label, cv=10, scoring='precision')\n",
    "        print('Precision', np.mean(precision), precision)\n",
    "        f1 = cross_validation.cross_val_score(hagupit_clf, hagupit_feature, hagupit_label, cv=10, scoring='f1')\n",
    "        print('F1', np.mean(f1), f1)\n",
    "\tIN.append(scores.mean())\n",
    "\t\t\t\n",
    "        \n",
    "\tprint('Train Hydb')\n",
    "        print('Utkd: ',hydb_clf.score(utkd_feature,utkd_label))\n",
    "        print('Sandy_Hook: ',hydb_clf.score(sandy_hook_feature,sandy_hook_label))\n",
    "        print('Hagupit: ',hydb_clf.score(hagupit_feature,hagupit_label))\n",
    "\tCR.append(hydb_clf.score(utkd_feature,utkd_label))\n",
    "\tCR.append(hydb_clf.score(sandy_hook_feature,sandy_hook_label))\n",
    "\tCR.append(hydb_clf.score(hagupit_feature,hagupit_label))\n",
    "\n",
    "        print('Train Utkd')\n",
    "        print('Hydb: ',utkd_clf.score(hydb_feature,hydb_label))\n",
    "        print('Sandy_Hook: ',utkd_clf.score(sandy_hook_feature,sandy_hook_label))\n",
    "        print('Hagupit: ',utkd_clf.score(hagupit_feature,hagupit_label))\n",
    "\tCR.append(utkd_clf.score(hydb_feature,hydb_label))\n",
    "\tCR.append(utkd_clf.score(sandy_hook_feature,sandy_hook_label))\n",
    "\tCR.append(utkd_clf.score(hagupit_feature,hagupit_label))\n",
    "\n",
    "        print('Train Sandy_Hook')\n",
    "        print('Hydb: ',sandy_hook_clf.score(hydb_feature,hydb_label))\n",
    "        print('Utkd: ',sandy_hook_clf.score(utkd_feature,utkd_label))\n",
    "        print('Hagupit: ',sandy_hook_clf.score(hagupit_feature,hagupit_label))\n",
    "\tCR.append(sandy_hook_clf.score(hydb_feature,hydb_label))\n",
    "\tCR.append(sandy_hook_clf.score(utkd_feature,utkd_label))\n",
    "\tCR.append(sandy_hook_clf.score(hagupit_feature,hagupit_label))\n",
    "\n",
    "        print('Train Hagupit')\n",
    "        print('Hydb: ',hagupit_clf.score(hydb_feature,hydb_label))\n",
    "        print('Utkd: ',hagupit_clf.score(utkd_feature,utkd_label))\n",
    "        print('Sandy_Hook: ',hagupit_clf.score(sandy_hook_feature,sandy_hook_label))\n",
    "\tCR.append(hagupit_clf.score(hydb_feature,hydb_label))\n",
    "\tCR.append(hagupit_clf.score(utkd_feature,utkd_label))\n",
    "\tCR.append(hagupit_clf.score(sandy_hook_feature,sandy_hook_label))\n",
    "\tprint('\\n\\n')\n",
    "\t\n",
    "\t########################################## Precision Recall F-score ###################################################\n",
    "\tutkd_predicted_label = hydb_clf.predict(utkd_feature)\n",
    "\tprint('Hydb -> utkd | sandy_hook | hagupit')\n",
    "\tprint('PRECISION: ',metrics.precision_score(utkd_label,utkd_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(utkd_label,utkd_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(utkd_label,utkd_predicted_label))\n",
    "\tPR.append(metrics.precision_score(utkd_label,utkd_predicted_label))\n",
    "\tRC.append(metrics.recall_score(utkd_label,utkd_predicted_label))\n",
    "\tFS.append(metrics.f1_score(utkd_label,utkd_predicted_label))\n",
    "\t\n",
    "\tsandy_hook_predicted_label = hydb_clf.predict(sandy_hook_feature)\n",
    "\tprint('PRECISION: ',metrics.precision_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "\tPR.append(metrics.precision_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "        RC.append(metrics.recall_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "        FS.append(metrics.f1_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "\t\n",
    "\thagupit_predicted_label = hydb_clf.predict(hagupit_feature)\n",
    "\tprint('PRECISION: ',metrics.precision_score(hagupit_label,hagupit_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(hagupit_label,hagupit_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(hagupit_label,hagupit_predicted_label))\n",
    "\tPR.append(metrics.precision_score(hagupit_label,hagupit_predicted_label))\n",
    "        RC.append(metrics.recall_score(hagupit_label,hagupit_predicted_label))\n",
    "        FS.append(metrics.f1_score(hagupit_label,hagupit_predicted_label))\n",
    "\n",
    "\tprint('\\n\\n')\n",
    "\t\n",
    "\t#########################################################################################################################\n",
    "\thydb_predicted_label = utkd_clf.predict(hydb_feature)\n",
    "\tprint('Utkd -> hydb | sandy_hook | hagupit')\n",
    "\tprint('PRECISION: ',metrics.precision_score(hydb_label,hydb_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(hydb_label,hydb_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(hydb_label,hydb_predicted_label))\n",
    "\tPR.append(metrics.precision_score(hydb_label,hydb_predicted_label))\n",
    "        RC.append(metrics.recall_score(hydb_label,hydb_predicted_label))\n",
    "        FS.append(metrics.f1_score(hydb_label,hydb_predicted_label))\n",
    "\t\n",
    "\tsandy_hook_predicted_label = utkd_clf.predict(sandy_hook_feature)\n",
    "\tprint('PRECISION: ',metrics.precision_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "\tPR.append(metrics.precision_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "        RC.append(metrics.recall_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "        FS.append(metrics.f1_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "\t\n",
    "\thagupit_predicted_label = utkd_clf.predict(hagupit_feature)\n",
    "\tprint('PRECISION: ',metrics.precision_score(hagupit_label,hagupit_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(hagupit_label,hagupit_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(hagupit_label,hagupit_predicted_label))\n",
    "\tPR.append(metrics.precision_score(hagupit_label,hagupit_predicted_label))\n",
    "        RC.append(metrics.recall_score(hagupit_label,hagupit_predicted_label))\n",
    "        FS.append(metrics.f1_score(hagupit_label,hagupit_predicted_label))\n",
    "\tprint('\\n\\n')\n",
    "\n",
    "\t###########################################################################################################################\n",
    "\thydb_predicted_label = sandy_hook_clf.predict(hydb_feature)\n",
    "\tprint('Sandy_hook -> hydb | utkd | hagupit')\n",
    "\tprint('PRECISION: ',metrics.precision_score(hydb_label,hydb_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(hydb_label,hydb_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(hydb_label,hydb_predicted_label))\n",
    "\tPR.append(metrics.precision_score(hydb_label,hydb_predicted_label))\n",
    "        RC.append(metrics.recall_score(hydb_label,hydb_predicted_label))\n",
    "        FS.append(metrics.f1_score(hydb_label,hydb_predicted_label))\n",
    "\t\n",
    "\tutkd_predicted_label = sandy_hook_clf.predict(utkd_feature)\n",
    "\tprint('PRECISION: ',metrics.precision_score(utkd_label,utkd_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(utkd_label,utkd_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(utkd_label,utkd_predicted_label))\n",
    "\tPR.append(metrics.precision_score(utkd_label,utkd_predicted_label))\n",
    "\tRC.append(metrics.recall_score(utkd_label,utkd_predicted_label))\n",
    "\tFS.append(metrics.f1_score(utkd_label,utkd_predicted_label))\n",
    "\t\n",
    "\thagupit_predicted_label = sandy_hook_clf.predict(hagupit_feature)\n",
    "\tprint('PRECISION: ',metrics.precision_score(hagupit_label,hagupit_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(hagupit_label,hagupit_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(hagupit_label,hagupit_predicted_label))\n",
    "\tPR.append(metrics.precision_score(hagupit_label,hagupit_predicted_label))\n",
    "        RC.append(metrics.recall_score(hagupit_label,hagupit_predicted_label))\n",
    "        FS.append(metrics.f1_score(hagupit_label,hagupit_predicted_label))\n",
    "\tprint('\\n\\n')\n",
    "\n",
    "\t############################################################################################################################\n",
    "\t\n",
    "\tprint('Hagupit -> hydb | utkd | sandy_hook')\n",
    "\thydb_predicted_label = hagupit_clf.predict(hydb_feature)\n",
    "\tprint('PRECISION: ',metrics.precision_score(hydb_label,hydb_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(hydb_label,hydb_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(hydb_label,hydb_predicted_label))\n",
    "\tPR.append(metrics.precision_score(hydb_label,hydb_predicted_label))\n",
    "        RC.append(metrics.recall_score(hydb_label,hydb_predicted_label))\n",
    "        FS.append(metrics.f1_score(hydb_label,hydb_predicted_label))\n",
    "\t\n",
    "\tutkd_predicted_label = hagupit_clf.predict(utkd_feature)\n",
    "\tprint('PRECISION: ',metrics.precision_score(utkd_label,utkd_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(utkd_label,utkd_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(utkd_label,utkd_predicted_label))\n",
    "\tPR.append(metrics.precision_score(utkd_label,utkd_predicted_label))\n",
    "\tRC.append(metrics.recall_score(utkd_label,utkd_predicted_label))\n",
    "\tFS.append(metrics.f1_score(utkd_label,utkd_predicted_label))\n",
    "\t\n",
    "\tsandy_hook_predicted_label = hagupit_clf.predict(sandy_hook_feature)\n",
    "\tprint('PRECISION: ',metrics.precision_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "\tPR.append(metrics.precision_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "        RC.append(metrics.recall_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "        FS.append(metrics.f1_score(sandy_hook_label,sandy_hook_predicted_label))\n",
    "\t\n",
    "\tprint(len(IN),len(CR),len(PR),len(RC),len(FS))\n",
    "        print('Average Indomain Accuracy: ',np.mean(IN))\n",
    "        print('Average Cross-domain Accuracy: ',np.mean(CR))\n",
    "        print('Average and std Precision: ',np.mean(PR),np.std(PR))\n",
    "        print('Average and std Recall: ',np.mean(RC),np.std(RC))\n",
    "        print('Average and std F-score: ',np.mean(FS),np.std(FS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 80p and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.53      0.49        17\n",
      "          1       0.38      0.31      0.34        16\n",
      "\n",
      "avg / total       0.42      0.42      0.42        33\n",
      "\n",
      "('Accuracy', 0.42424242424242425)\n",
      "('Macro-recall', 0.4209558823529412)\n",
      "('Macro-precision', 0.4173076923076923)\n",
      "('Macro-f1-score', 0.4156570363466915)\n",
      "('Micro-recall', 0.42424242424242425)\n",
      "('Micro-precision', 0.42424242424242425)\n",
      "('Micro-f1-score', 0.4242424242424243)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on 11-May-2015\n",
    "\n",
    "@author: Koustav\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "from textblob import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn import svm\n",
    "from sklearn.svm import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "import gzip\n",
    "from happyfuntokenizing import *\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "#from happyfuntokenizing import *\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "mycompile = lambda pat:  re.compile(pat,  re.UNICODE)\n",
    "\n",
    "PRONOUN_PATH = '../../Lexical_resources/classifier_dictionary/english_pronoun.txt'\n",
    "WHWORD_PATH = '../classifier_dictionary/english_whwords.txt'\n",
    "SLANG_PATH = '../classifier_dictionary/english_swear.txt'\n",
    "INTENSIFIER_PATH = '../classifier_dictionary/english_intensifier.txt'\n",
    "SUBJECTIVE_PATH = '../classifier_dictionary/subjclueslen1-HLTEMNLP05.tff'\n",
    "EVENT_PATH = '../classifier_dictionary/english_nonsituational_phrase.txt'\n",
    "MODAL_VERB_PATH = '../classifier_dictionary/english_modal_verb.txt'\n",
    "RELIGION_PATH = '../classifier_dictionary/communal_race.txt'\n",
    "#NONSIT_PATH = 'Common_nonsituational_word.txt'\n",
    "#OPINION_HASHTAG_PATH = '/home/krudra/twitter_code/shared/language_overlap/code_mix_pitch/devanagari/devanagari_hashtag_opinion.txt'\n",
    "#MENTION_PATH = '/home/krudra/twitter_code/shared/language_overlap/code_mix_pitch/devanagari/news_mention.txt'\n",
    "\n",
    "#TRAIN1 = '../classification_4.3/fragmented_train_data/hydb_fragment_train.txt'\n",
    "#TRAIN2 = '../classification_4.3/fragmented_train_data/utkd_fragment_train.txt'\n",
    "#TRAIN3 = '../classification_4.3/fragmented_train_data/sandy_hook_fragment_train.txt'\n",
    "#TRAIN4 = '../classification_4.3/fragmented_train_data/hagupit_fragment_train.txt'\n",
    "#TRAIN1=\"../dataset4may/SMERP/RetrievedSMERP/level-1/TotalSMERP_T31.tsv\"\n",
    "\n",
    "'''TRAIN1 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hydb_RAW_CLASS.txt'\n",
    "TRAIN2 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/utkd_RAW_CLASS.txt'\n",
    "TRAIN3 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/sandy_hook_RAW_CLASS.txt'\n",
    "TRAIN4 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hagupit_RAW_CLASS.txt' '''\n",
    "\n",
    "'''TRAIN1 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hydb_balance_RAW.txt'\n",
    "TRAIN2 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/utkd_balance_RAW.txt'\n",
    "TRAIN3 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/sandy_hook_balance_RAW.txt'\n",
    "TRAIN4 = '/home/krudra/summarization/codetest/TWEB_REVISION/Training_Data/hagupit_balance_RAW.txt' '''\n",
    "\n",
    "\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "TAGGER_PATH = '../../Lexical_resources/ark-tweet-nlp-0.3.2/'\n",
    "\n",
    "PRONOUN = {}\n",
    "WHWORD = {}\n",
    "SLANG = {}\n",
    "INTENSIFIER = {}\n",
    "SUBJECTIVE = {}\n",
    "EVENT = {}\n",
    "MODAL = {}\n",
    "RELIGION = {}\n",
    "\n",
    "def READ_FILES():\n",
    "\n",
    "\tfp = open(PRONOUN_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tPRONOUN[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(INTENSIFIER_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tINTENSIFIER[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(WHWORD_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tWHWORD[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(SLANG_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tSLANG[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\tfp = open(EVENT_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tEVENT[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\t'''fp = open(NONSIT_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tEVENT[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close() '''\n",
    "\t\n",
    "\tfp = open(MODAL_VERB_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tMODAL[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\t\n",
    "\tfp = open(RELIGION_PATH,'r')\n",
    "        for l in fp:\n",
    "\t\tRELIGION[l.strip(' \\t\\n\\r').lower()] = 1\n",
    "        fp.close()\n",
    "\n",
    "\tfp = open(SUBJECTIVE_PATH,'r')\n",
    "        for l in fp:\n",
    "                wl = l.split()\n",
    "                x = wl[0].split('=')[1].strip(' \\t\\n\\r')\n",
    "                if x=='strongsubj':\n",
    "                        y = wl[2].split('=')[1].strip(' \\t\\n\\r')\n",
    "                        SUBJECTIVE[y.lower()] = 1\n",
    "\tfp.close()\n",
    "\n",
    "############################ This Functions are used #############################################\n",
    "\n",
    "def emoticons(s):\n",
    "        return len(re.findall(u'[\\U0001f600-\\U0001f60f\\U0001f617-\\U0001f61d\\U0001f632\\U0001f633\\U0001f638-\\U0001f63e\\U0001f642\\U0001f646-\\U0001f64f\\U0001f612\\U0001f613\\U0001f615\\U0001f616\\U0001f61e-\\U0001f629\\U0001f62c\\U0001f62d\\U0001f630\\U0001f631\\U0001f636\\U0001f637\\U0001f63c\\U0001f63f-\\U0001f641\\U0001f64d]', s))\n",
    "\n",
    "def smileys(s):\n",
    "        return len(re.findall(r':\\-\\)|:[\\)\\]\\}]|:[dDpP]|:3|:c\\)|:>|=\\]|8\\)|=\\)|:\\^\\)|:\\-D|[xX8]\\-?D|=\\-?D|=\\-?3|B\\^D|:\\'\\-?\\)|>:\\[|:\\-?\\(|:\\-?c|:\\-?<|:\\-?\\[|:\\{|;\\(|:\\-\\|\\||:@|>:\\(|:\\'\\-?\\(|D:<?|D[8;=X]|v.v|D\\-\\':|>:[\\/]|:\\-[./]|:[\\/LS]|=[\\/L]|>.<|:\\$|>:\\-?\\)|>;\\)|\\}:\\-?\\)|3:\\-?\\)|\\(>_<\\)>?|^_?^;|\\(#\\^.\\^#\\)|[Oo]_[Oo]|:\\-?o',s))\n",
    "\n",
    "def getNumberOfElongatedWords(s):\n",
    "    return len(re.findall('([a-zA-Z])\\\\1{2,}', s))\n",
    "    \n",
    "def pronoun(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif PRONOUN.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def exclamation(s):\n",
    "\tc = len(re.findall(r\"[!]\", s))\n",
    "\tif c>=1:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def question(s):\n",
    "\treturn len(re.findall(r\"[?]\", s))\n",
    "\n",
    "def intensifier(sen):\n",
    "\n",
    "\tcount = 0\n",
    "\tfor x in sen:\n",
    "\t\tif INTENSIFIER.__contains__(x)==True:\n",
    "\t\t\tcount+=1\n",
    "\t\t\t#return 1\n",
    "\tif count>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def whword(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif WHWORD.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def religion(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif RELIGION.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def slang(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif SLANG.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def event_phrase(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "\t\tif EVENT.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def getHashtagopinion(sen):\n",
    "\tfp = codecs.open(OPINION_HASHTAG_PATH,'r','utf-8')\n",
    "\ttemp = set([])\n",
    "\tfor l in fp:\n",
    "\t\ttemp.add(l.strip(' \\t\\n\\r').lower())\n",
    "\tfp.close()\n",
    "\n",
    "\tcur_hash = set([])\n",
    "\tfor x in sen:\n",
    "\t\tif x.startswith('#')==True:\n",
    "\t\t\tcur_hash.add(x.strip(' \\t\\n\\r').lower())\n",
    "\tsize = len(temp.intersection(cur_hash))\n",
    "\tif size>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def numeral(temp):\n",
    "\tc = 0\n",
    "\tfor x in temp:\n",
    "\t\tif x.isdigit()==True:\n",
    "\t\t\tc+=1\n",
    "\treturn c\n",
    "\n",
    "def modal(sen):\n",
    "\tfor x in sen:\n",
    "\t\tif MODAL.__contains__(x)==True:\n",
    "\t\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def subjectivity(sen):\n",
    "\t\n",
    "        c = 0\n",
    "        for x in sen:\n",
    "                if SUBJECTIVE.__contains__(x)==True:\n",
    "                        c+=1\n",
    "        tot = len(sen) + 4.0 - 4.0 - 1.0\n",
    "        num = c + 4.0 - 4.0\n",
    "\ttry:\n",
    "        \ts = round(num/tot,4)\n",
    "\texcept:\n",
    "\t\ts = 0\n",
    "\tif c>0:\n",
    "\t\treturn c\n",
    "\treturn 0\n",
    "\t#print(s)\n",
    "\t#return s\n",
    "\n",
    "#HashTagSentimentUnigrams(\"Oh wrestle\", \"NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#HashTagSentimentUnigrams(\"Oh no wrestle\", \"Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#print negatedContextCount(\"this is not for me\")\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "\n",
    "\tREAD_FILES()\n",
    "\ttok = Tokenizer(preserve_case=False)\n",
    "\tglobal hydb_clf\n",
    "\tfo = open('temp.txt','w')\n",
    "\t#fp = open(TRAIN1,'r')\n",
    "\tfor l in X_train:\n",
    "\t     #wl = l.split('\\t')\n",
    "\t     fo.write(l.encode('utf-8').strip(' \\t\\n\\r') + '\\n')\n",
    "\t#fp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\t#fs = open(TRAIN1,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\thydb_feature = []\n",
    "\thydb_label = []\n",
    "\tfor l,k,j in zip(fp,X_train,y_train):\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\t#row = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(i)\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\t#print(unigram)\n",
    "\t\t\t#print(temp)\n",
    "\t\t\t#print(Ngram)\n",
    "\t\t\t#sys.exit(0)\n",
    "\t\t\tE = exclamation(k)\n",
    "\t\t\tQ = question(k)\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(k)\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\thydb_feature.append(t)\n",
    "\t\t\thydb_label.append(int(j))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\t#fs.close()\n",
    "\t\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "        hydb_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #hydb_clf = svm.SVC(kernel='rbf')\n",
    "        #hydb_clf = RandomForestClassifier()\n",
    "        #hydb_clf = svm.LinearSVC()\n",
    "        #hydb_clf = LogisticRegression()\n",
    "        #hydb_clf = BernoulliNB()\n",
    "        hydb_clf.fit(hydb_feature,hydb_label)\n",
    "        \n",
    "\n",
    "\tfo = open('temp.txt','w')\n",
    "\t#fp = open(TRAIN2,'r')\n",
    "\tfor l in X_test:\n",
    "\t\t#wl = l.split('\\t')\n",
    "\t\tfo.write(l.encode('utf-8').strip(' \\t\\n\\r') + '\\n')\n",
    "\t#fp.close()\n",
    "\tfo.close()\n",
    "\n",
    "\tcommand = TAGGER_PATH + '/./runTagger.sh --output-format conll temp.txt > tagfile.txt'\n",
    "\tos.system(command)\n",
    "\t\n",
    "\tfp = open('tagfile.txt','r')\n",
    "\t#fs = open(TRAIN2,'r')\n",
    "\ts = ''\n",
    "\tN = 0\n",
    "\tutkd_feature = []\n",
    "\tutkd_label = []\n",
    "\tfor l,k,j in zip(fp,X_test,y_test):\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>1:\n",
    "\t\t\tword = wl[0].strip(' \\t\\n\\r').lower()\n",
    "\t\t\ttag = wl[1].strip(' \\t\\n\\r')\n",
    "\t\t\tif tag=='N':\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tw = lmtzr.lemmatize(word)\n",
    "\t\t\t\t\tword = w\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tpass\n",
    "\t\t\telif tag=='V':\n",
    "\t\t\t\ttry:\n",
    "                                \tw = Word(word)\n",
    "                                \tx = w.lemmatize(\"v\")\n",
    "\t\t\t\t\tword = x\n",
    "\t\t\t\texcept Exception as e:\n",
    "                                \tpass\n",
    "\t\t\telif tag=='$':\n",
    "\t\t\t\tN+=1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\ttry:\n",
    "\t\t\t\ts = s + word + ' '\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\tunigram = tok.tokenize(s)\n",
    "\t\t\tbigram = []\n",
    "\t\t\ttrigram = []\n",
    "\t\t\tif len(unigram)>1:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-1,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1]\n",
    "\t\t\t\t\tbigram.append(s)\n",
    "\t\t\tif len(unigram)>2:\n",
    "\t\t\t\tfor i in range(0,len(unigram)-2,1):\n",
    "\t\t\t\t\ts = unigram[i] + ' ' + unigram[i+1] + ' ' + unigram[i+2]\n",
    "\t\t\t\t\ttrigram.append(s)\n",
    "\t\t\tNgram = unigram + bigram + trigram\n",
    "\t\t\t#row = fs.readline().split('\\t')\n",
    "\t\t\ttemp = tok.tokenize(i)\n",
    "\t\t\tNew = []\n",
    "\t\t\tif len(temp)>1:\n",
    "\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\tif len(temp)>2:\n",
    "\t\t\t\tfor i in range(0,len(temp)-2,1):\n",
    "\t\t\t\t\ts = temp[i] + ' ' + temp[i+1] + ' ' + temp[i+2]\n",
    "\t\t\t\t\tNew.append(s)\n",
    "\t\t\t\n",
    "\t\t\tNgram = Ngram + temp + New\n",
    "\t\t\tNgram = set(Ngram)\n",
    "\t\t\tNgram = list(Ngram)\n",
    "\t\t\tE = exclamation(k)\n",
    "\t\t\tQ = question(k)\n",
    "\t\t\tM = modal(Ngram)\n",
    "\t\t\tI = intensifier(Ngram)\n",
    "\t\t\tW = whword(Ngram)\n",
    "\t\t\tEP = event_phrase(Ngram)\n",
    "\t\t\tS = subjectivity(temp)\n",
    "\t\t\tSG = slang(Ngram)\n",
    "\t\t\tP = pronoun(Ngram)\n",
    "\t\t\tEL = getNumberOfElongatedWords(k)\n",
    "\t\t\tRL = religion(Ngram)\n",
    "\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t#SM = smileys(row[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EL]\n",
    "\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\tutkd_feature.append(t)\n",
    "\t\t\tutkd_label.append(int(j))\n",
    "\t\t\tN = 0\n",
    "\t\t\ts = ''\n",
    "\tfp.close()\n",
    "\t#fs.close()\n",
    "        \n",
    "\t#utkd_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #utkd_clf = svm.SVC(kernel='rbf')\n",
    "        #utkd_clf = RandomForestClassifier()\n",
    "        #utkd_clf = svm.LinearSVC()\n",
    "        #utkd_clf = LogisticRegression()\n",
    "        #utkd_clf = BernoulliNB()\n",
    "        #hybd_predict=hybd_clf.predict(utkd_feature)\n",
    "        #print classification_report(utkd_label,hybd_predict)\n",
    "\thydb_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "        #hydb_clf = svm.SVC(kernel='rbf')\n",
    "        #hydb_clf = RandomForestClassifier()\n",
    "        #hydb_clf = svm.LinearSVC()\n",
    "        #hydb_clf = LogisticRegression()\n",
    "        #hydb_clf = BernoulliNB()\n",
    "\tX=hydb_clf.fit(hydb_feature,hydb_label)\n",
    "\thybd_predict=X.predict(utkd_feature)\n",
    "\ty_test=utkd_label\n",
    "\ty_pred_class=hybd_predict\n",
    "\tprint classification_report(utkd_label,hybd_predict)\n",
    "\t#print metrics.classification_report(y_test, y_pred_class)\n",
    "#from sklearn import metrics\n",
    "\tprint (\"Accuracy\",metrics.accuracy_score(y_test,y_pred_class))\n",
    "#print (\"ROC_AUC_Score\",roc_auc_score(y_test, y_pred_class))\n",
    "#print confusion_matrix(y_test,y_pred_class)\n",
    "\tprint(\"Macro-recall\",recall_score(y_test, y_pred_class, average=\"macro\"))  \n",
    "\tprint(\"Macro-precision\",precision_score(y_test, y_pred_class, average=\"macro\"))\n",
    "\tprint(\"Macro-f1-score\",f1_score(y_test, y_pred_class, average=\"macro\"))\n",
    "\tprint(\"Micro-recall\",recall_score(y_test, y_pred_class, average=\"micro\"))  \n",
    "\tprint(\"Micro-precision\",precision_score(y_test, y_pred_class, average=\"micro\"))\n",
    "\tprint(\"Micro-f1-score\",f1_score(y_test, y_pred_class, average=\"micro\"))\n",
    "        #scores = cross_validation.cross_val_score(utkd_clf,utkd_feature,utkd_label,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
