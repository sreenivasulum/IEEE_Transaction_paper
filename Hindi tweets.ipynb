{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import *\n",
    "from sklearn import cross_validation\n",
    "import gzip\n",
    "#from happyfuntokenizing import *\n",
    "\n",
    "mycompile = lambda pat:  re.compile(pat,  re.UNICODE)\n",
    "\n",
    "PRONOUN_PATH = '/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Lexical_resources/devanagari/devanagari_personal_pronoun.txt'\n",
    "WHWORD_PATH = '/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Lexical_resources/devanagari/devanagari_wh_words.txt'\n",
    "SLANG_PATH = '/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Lexical_resources/devanagari/devanagari_slang.txt'\n",
    "INTENSIFIER_PATH = '/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Lexical_resources/devanagari/devanagari_intensifier.txt'\n",
    "SUBJECTIVE_PATH = '/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Lexical_resources/devanagari/devanagari_subjective.txt'\n",
    "EVENT_PATH = '/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Lexical_resources/devanagari/devanagari_domain_list.txt'\n",
    "RELIGION_PATH = '/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Lexical_resources/devanagari/devanagari_religion.txt'\n",
    "MODAL_VERB_PATH = '/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Lexical_resources/devanagari/devanagari_modal_verb.txt'\n",
    "\n",
    "PRONOUN = {}\n",
    "INTENSIFIER = {}\n",
    "WHWORD = {}\n",
    "SLANG = {}\n",
    "EVENT = {}\n",
    "MODAL = {}\n",
    "RELIGION = {}\n",
    "SUBJECTIVE = {}\n",
    "\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "#TAGGER_PATH = '/home/krudra/twitter_code/term_project/ark-tweet-nlp-0.3.2/'\n",
    "\n",
    "def getNumberofHashTags(s):\n",
    "    return len(re.findall(r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\", s))\n",
    "\n",
    "def getNumberOfCaps(s):\n",
    "    return len(re.findall(\"[A-Z]+\", s))\n",
    "\n",
    "def getNumberOfPunctuations(s):\n",
    "    punctuations= string.punctuation\n",
    "    punctuations = re.sub(\"[#@]\", \"\", punctuations)\n",
    "    #print punctuations\n",
    "    return len(re.findall(\"['%s']\"%punctuations, s))\n",
    "\n",
    "#def getNumberOfSmileys(s):\n",
    "#    return len(re.findall(emoticon_string, s))\n",
    "\n",
    "def getNumberOfPositiveSmileys(s):\n",
    "    NormalEyes = r'[:=]'\n",
    "    #Wink = r'[;]'\n",
    "    NoseArea = r'(|o|O|-)'   ## rather tight precision, \\S might be reasonable...\n",
    "    HappyMouths = r'[D\\)\\]]'\n",
    "    #SadMouths = r'[\\(\\[]'\n",
    "    #Tongue = r'[pP]'\n",
    "    #OtherMouths = r'[doO/\\\\]'  # remove forward slash if http://'s aren't cleaned\n",
    "    Happy_RE =  mycompile( '(\\^_\\^|' + NormalEyes + NoseArea + HappyMouths + ')')\n",
    "    #Sad_RE = mycompile(NormalEyes + NoseArea + SadMouths)\n",
    "    \n",
    "#     Wink_RE = mycompile(Wink + NoseArea + HappyMouths)\n",
    "#     Tongue_RE = mycompile(NormalEyes + NoseArea + Tongue)\n",
    "#     Other_RE = mycompile( '('+NormalEyes+'|'+Wink+')'  + NoseArea + OtherMouths )\n",
    "#     \n",
    "#     Emoticon = (\n",
    "#         \"(\"+NormalEyes+\"|\"+Wink+\")\" +\n",
    "#         NoseArea + \n",
    "#         \"(\"+Tongue+\"|\"+OtherMouths+\"|\"+SadMouths+\"|\"+HappyMouths+\")\"\n",
    "#     )\n",
    "#     Emoticon_RE = mycompile(Emoticon)\n",
    "# \n",
    "#     Emoticon_RE = \"|\".join([Happy_RE,Sad_RE,Wink_RE,Tongue_RE,Other_RE])\n",
    "#     Emoticon_RE = mycompile(Emoticon_RE)\n",
    "    return len(re.findall(Happy_RE, s))    \n",
    "\n",
    "def getNumberOfNegativeSmileys(s):\n",
    "    NormalEyes = r'[:=]'\n",
    "    NoseArea = r'(|o|O|-)'   ## rather tight precision, \\S might be reasonable...\n",
    "    SadMouths = r'[\\(\\[]'\n",
    "    Sad_RE = mycompile(NormalEyes + NoseArea + SadMouths)\n",
    "    return len(re.findall(Sad_RE, s))    \n",
    "\n",
    "#print getNumberofHashTags(\"#unni_k #win AB\")\n",
    "#print getNumberOfCaps(\"#unni_k #win AB CC\")\n",
    "#print getNumberOfPunctuations(\"!!?\")\n",
    "#print getNumberOfSmileys(\":):P\")\n",
    "#print getNumberOfElongatedWords(\"sooo muchhhhh\")\n",
    "#print getNumberOfPositiveSmileys(\":) :( :P :-) :D :P\")\n",
    "#print getNumberOfNegativeSmileys(\":( :) :(\")\n",
    "\n",
    "def isLastCharacterPunctuation(s):\n",
    "    if(s[-1]=='!' or s[-1]=='?'):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def isLastTokenPositiveEmoticon(s):\n",
    "    lastToken = s.split()[-1]\n",
    "    if(getNumberOfPositiveSmileys(lastToken)>=1):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def isLastTokenNegativeEmoticon(s):\n",
    "    lastToken = s.split()[-1]\n",
    "    if(getNumberOfNegativeSmileys(lastToken)>=1):\n",
    "        return 1\n",
    "    return 0        \n",
    "\n",
    "def getNumberOfPositiveWords(s):\n",
    "    with open(\"positiveWords.txt\", \"r\") as f:\n",
    "        wordsFromS = set(s.split())\n",
    "        wordsFromFile = set(x.rstrip() for x in f.readlines())\n",
    "        #print wordsFromS\n",
    "        return len(wordsFromFile.intersection(wordsFromS))\n",
    "\n",
    "def getNumberOfNegativeWords(s):\n",
    "    with open(\"negativeWords.txt\", \"r\") as f:\n",
    "        wordsFromS = set(s.split())\n",
    "        wordsFromFile = set(x.rstrip() for x in f.readlines())\n",
    "        #print wordsFromS\n",
    "        return len(wordsFromFile.intersection(wordsFromS))\n",
    "\n",
    "def READ_FILES():\n",
    "\n",
    "        fp = codecs.open(PRONOUN_PATH,'r','utf-8')\n",
    "        for l in fp:\n",
    "                PRONOUN[l.strip(' \\t\\n\\r')] = 1\n",
    "        fp.close()\n",
    "\n",
    "        fp = codecs.open(INTENSIFIER_PATH,'r','utf-8')\n",
    "        for l in fp:\n",
    "                INTENSIFIER[l.strip(' \\t\\n\\r')] = 1\n",
    "        fp.close()\n",
    "\n",
    "        fp = codecs.open(WHWORD_PATH,'r','utf-8')\n",
    "        for l in fp:\n",
    "                WHWORD[l.strip(' \\t\\n\\r')] = 1\n",
    "        fp.close()\n",
    "\n",
    "        fp = codecs.open(SLANG_PATH,'r','utf-8')\n",
    "        for l in fp:\n",
    "                SLANG[l.strip(' \\t\\n\\r')] = 1\n",
    "        fp.close()\n",
    "\n",
    "        fp = codecs.open(EVENT_PATH,'r','utf-8')\n",
    "        for l in fp:\n",
    "                EVENT[l.strip(' \\t\\n\\r')] = 1\n",
    "        fp.close()\n",
    "\n",
    "        fp = codecs.open(MODAL_VERB_PATH,'r','utf-8')\n",
    "        for l in fp:\n",
    "                MODAL[l.strip(' \\t\\n\\r')] = 1\n",
    "        fp.close()\n",
    "\n",
    "        fp = codecs.open(RELIGION_PATH,'r','utf-8')\n",
    "        for l in fp:\n",
    "                RELIGION[l.strip(' \\t\\n\\r')] = 1\n",
    "        fp.close()\n",
    "\n",
    "        fp = codecs.open(SUBJECTIVE_PATH,'r','utf-8')\n",
    "        for l in fp:\n",
    "\t\tSUBJECTIVE[l.strip(' \\t\\n\\r')] = 1\n",
    "        fp.close()\n",
    "\n",
    "\n",
    "#getNumberOfNegativeWords(\"aa b\")    \n",
    "\n",
    "############################ This Functions are used #############################################\n",
    "def emoticons(s):\n",
    "        return len(re.findall(u'[\\U0001f600-\\U0001f60f\\U0001f617-\\U0001f61d\\U0001f632\\U0001f633\\U0001f638-\\U0001f63e\\U0001f642\\U0001f646-\\U0001f64f\\U0001f612\\U0001f613\\U0001f615\\U0001f616\\U0001f61e-\\U0001f629\\U0001f62c\\U0001f62d\\U0001f630\\U0001f631\\U0001f636\\U0001f637\\U0001f63c\\U0001f63f-\\U0001f641\\U0001f64d]', s))\n",
    "\n",
    "def smileys(s):\n",
    "        return len(re.findall(r':\\-\\)|:[\\)\\]\\}]|:[dDpP]|:3|:c\\)|:>|=\\]|8\\)|=\\)|:\\^\\)|:\\-D|[xX8]\\-?D|=\\-?D|=\\-?3|B\\^D|:\\'\\-?\\)|>:\\[|:\\-?\\(|:\\-?c|:\\-?<|:\\-?\\[|:\\{|;\\(|:\\-\\|\\||:@|>:\\(|:\\'\\-?\\(|D:<?|D[8;=X]|v.v|D\\-\\':|>:[\\/]|:\\-[./]|:[\\/LS]|=[\\/L]|>.<|:\\$|>:\\-?\\)|>;\\)|\\}:\\-?\\)|3:\\-?\\)|\\(>_<\\)>?|^_?^;|\\(#\\^.\\^#\\)|[Oo]_[Oo]|:\\-?o',s))\n",
    "\n",
    "def getNumberOfElongatedWords(s):\n",
    "    return len(re.findall('([a-zA-Z])\\\\1{2,}', s))\n",
    "    \n",
    "def pronoun(sen):\n",
    "\n",
    "\tfor x in sen:\n",
    "                if PRONOUN.__contains__(x)==True:\n",
    "                        return 1\n",
    "        return 0\n",
    "\n",
    "def exclamation(s):\n",
    "\tc = len(re.findall(r\"[!]\", s))\n",
    "\tif c>=1:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def question(s):\n",
    "\treturn len(re.findall(r\"[?]\", s))\n",
    "\n",
    "def intensifier(sen):\n",
    "\tcount = 0\n",
    "        for x in sen:\n",
    "                if INTENSIFIER.__contains__(x)==True:\n",
    "                        count+=1\n",
    "                        #return 1\n",
    "        if count>0:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "def religion(sen):\n",
    "\n",
    "        for x in sen:\n",
    "                if RELIGION.__contains__(x)==True:\n",
    "                        return 1\n",
    "        return 0\n",
    "\n",
    "def whword(sen):\n",
    "\n",
    "\tfor x in sen:\n",
    "                if WHWORD.__contains__(x)==True:\n",
    "                        return 1\n",
    "        return 0\n",
    "\n",
    "def slang(sen):\n",
    "\n",
    "\tfor x in sen:\n",
    "                if SLANG.__contains__(x)==True:\n",
    "                        return 1\n",
    "        return 0\n",
    "\n",
    "def event_phrase(sen):\n",
    "\t\n",
    "\tfor x in sen:\n",
    "                if EVENT.__contains__(x)==True:\n",
    "                        return 1\n",
    "        return 0\n",
    "\n",
    "def getHashtagopinion(sen):\n",
    "\tfp = codecs.open(OPINION_HASHTAG_PATH,'r','utf-8')\n",
    "\ttemp = set([])\n",
    "\tfor l in fp:\n",
    "\t\ttemp.add(l.strip(' \\t\\n\\r').lower())\n",
    "\tfp.close()\n",
    "\n",
    "\tcur_hash = set([])\n",
    "\tfor x in sen:\n",
    "\t\tif x.startswith('#')==True:\n",
    "\t\t\tcur_hash.add(x.strip(' \\t\\n\\r').lower())\n",
    "\tsize = len(temp.intersection(cur_hash))\n",
    "\tif size>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def numeral(temp):\n",
    "\tc = 0\n",
    "\tfor x in temp:\n",
    "\t\tif x.isdigit()==True:\n",
    "\t\t\tc+=1\n",
    "\treturn c\n",
    "\n",
    "def getNewsMention(sen):\n",
    "\tfp = codecs.open(MENTION_PATH,'r','utf-8')\n",
    "\ttemp = set([])\n",
    "\tfor l in fp:\n",
    "\t\ttemp.add(l.strip(' \\t\\n\\r').lower())\n",
    "\tfp.close()\n",
    "\n",
    "\tcur_men = set([])\n",
    "\tfor x in sen:\n",
    "\t\tif x.startswith('@')==True:\n",
    "\t\t\tcur_men.add(x.strip(' \\t\\n\\r').lower())\n",
    "\tsize = len(temp.intersection(cur_men))\n",
    "\tif size>0:\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\t\n",
    "def modal(sen):\n",
    "\n",
    "\tfor x in sen:\n",
    "                if MODAL.__contains__(x)==True:\n",
    "                        return 1\n",
    "        return 0\n",
    "\n",
    "def subjectivity(sen):\n",
    "\tc = 0\n",
    "        for x in sen:\n",
    "                if SUBJECTIVE.__contains__(x)==True:\n",
    "                        c+=1\n",
    "        tot = len(sen) + 4.0 - 4.0 - 1.0\n",
    "        num = c + 4.0 - 4.0\n",
    "        try:\n",
    "                s = round(num/tot,4)\n",
    "        except:\n",
    "                s = 0\n",
    "        if c>0:\n",
    "                return c\n",
    "        return 0\n",
    "\n",
    "######################## Upto this part used #############################################################3\n",
    "\n",
    "#HashTagSentimentUnigrams(\"Oh wrestle\", \"NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#HashTagSentimentUnigrams(\"Oh no wrestle\", \"Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt\")\n",
    "#print negatedContextCount(\"this is not for me\")\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "\n",
    "\tREAD_FILES()\n",
    "\t'''org_tweet = {}\n",
    "\tfp = codecs.open(sys.argv[1],'r','utf-8')\n",
    "\tfor l in fp:\n",
    "\t\twl = l.split('\\t')\n",
    "\t\ttid = wl[1].strip(' \\t\\n\\r')\n",
    "\t\ts = wl[3].strip(' \\t\\n\\r').split()\n",
    "\t\ttext = ''\n",
    "\t\tfor x in s:\n",
    "\t\t\tif len(re.findall(r'[a-zA-Z]+',x))>0:\n",
    "\t\t\t\tpass\n",
    "\t\t\telse:\n",
    "\t\t\t\ttext = text + x + ' '\n",
    "\t\t#text = wl[3].strip(' \\t\\n\\r')\n",
    "\t\torg_tweet[tid] = text.strip()\n",
    "\tfp.close()'''\n",
    "\n",
    "\tIN = []\n",
    "\tCR = []\n",
    "\tPR = []\n",
    "\tRC = []\n",
    "\tFS = []\n",
    "\t\n",
    "\n",
    "\tfs = codecs.open(sys.argv[2],'r','utf-8')\n",
    "\tfp = codecs.open(sys.argv[1],'r','utf-8')\n",
    "\tnepal_feature = []\n",
    "\tnepal_label = []\n",
    "\tcount = 0\n",
    "\ttemp = []\n",
    "\tN = 0\n",
    "\t#temp_raw = []\n",
    "\tfor l in fs:\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>2:\n",
    "\t\t\tif wl[2].strip(' \\t\\n\\r')!='SYM':\n",
    "\t\t\t\ttemp.append(wl[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t\tif wl[2].strip(' \\t\\n\\r')=='QC':\n",
    "\t\t\t\t\tN+=1\n",
    "\t\telse:\n",
    "\t\t\tif wl[0].strip(' \\t\\n\\r')=='</s>':\n",
    "\t\t\t\trow = fp.readline().split('\\t')\n",
    "\t\t\t\tbigram = []\n",
    "\t\t\t\tif len(temp)>1:\n",
    "\t\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\t\tbigram.append(s)\n",
    "\t\t\t\ttemp = temp + bigram\n",
    "\t\t\t\t#row = XL[3].strip(' \\t\\n\\r')\n",
    "\t\t\t\t#temp = row[3].split()\n",
    "\t\t\t\t#N = numeral(temp)\n",
    "\t\t\t\tE = exclamation(row[3])\n",
    "\t\t\t\tQ = question(row[3])\n",
    "\t\t\t\tM = modal(temp)\n",
    "\t\t\t\tI = intensifier(temp)\n",
    "\t\t\t\tW = whword(temp)\n",
    "\t\t\t\tEP = event_phrase(temp)\n",
    "\t\t\t\tS = subjectivity(temp)\n",
    "\t\t\t\tSG = slang(temp)\n",
    "\t\t\t\tRL = religion(temp)\n",
    "\t\t\t\tP = pronoun(temp)\n",
    "\t\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t\t#SM = smileys(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\t\t#t = [N,E,Q,M,I,W,S,P,SG,RL]\n",
    "\t\t\t\tnepal_feature.append(t)\n",
    "\t\t\t\tnepal_label.append(int(row[4]))\n",
    "\t\t\t\tcount+=1\n",
    "\t\t\t\t'''if count==51:\n",
    "\t\t\t\t\tprint(row[1].strip(' \\t\\n\\r'))\n",
    "\t\t\t\t\tprint(t)\n",
    "\t\t\t\t\tsys.exit(0)'''\n",
    "\t\t\t\ttemp = []\n",
    "\t\t\t\tN = 0\n",
    "\t\t\t\n",
    "\n",
    "\tfp.close()\n",
    "\tfs.close()\n",
    "\n",
    "\tnepal_clf = svm.SVC()\n",
    "\t#nepal_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "\t#nepal_clf = svm.SVC(kernel='linear')\n",
    "\t#nepal_clf = svm.LinearSVC()\n",
    "\t#nepal_clf = LogisticRegression()\n",
    "\t#nepal_clf = BernoulliNB()\n",
    "\t#nepal_clf = RandomForestClassifier()\n",
    "\tnepal_clf.fit(nepal_feature,nepal_label)\n",
    "\tscores = cross_validation.cross_val_score(nepal_clf,nepal_feature,nepal_label,cv=10)\n",
    "\tprint(scores.mean(),scores.std())\n",
    "\t#clf.cross_validation(feature,label)\n",
    "\tIN.append(scores.mean())\n",
    "\t\n",
    "\n",
    "\tfs = codecs.open(sys.argv[4],'r','utf-8')\n",
    "\tfp = codecs.open(sys.argv[3],'r','utf-8')\n",
    "\tharda_feature = []\n",
    "\tharda_label = []\n",
    "\tcount = 0\n",
    "\ttemp = []\n",
    "\tN = 0\n",
    "\t#temp_raw = []\n",
    "\tfor l in fs:\n",
    "\t\twl = l.split('\\t')\n",
    "\t\tif len(wl)>2:\n",
    "\t\t\tif wl[2].strip(' \\t\\n\\r')!='SYM':\n",
    "\t\t\t\ttemp.append(wl[0].strip(' \\t\\n\\r'))\n",
    "\t\t\t\tif wl[2].strip(' \\t\\n\\r')=='QC':\n",
    "\t\t\t\t\tN+=1\n",
    "\t\telse:\n",
    "\t\t\tif wl[0].strip(' \\t\\n\\r')=='</s>':\n",
    "\t\t\t\trow = fp.readline().split('\\t')\n",
    "\t\t\t\tbigram = []\n",
    "\t\t\t\tif len(temp)>1:\n",
    "\t\t\t\t\tfor i in range(0,len(temp)-1,1):\n",
    "\t\t\t\t\t\ts = temp[i] + ' ' + temp[i+1]\n",
    "\t\t\t\t\t\tbigram.append(s)\n",
    "\t\t\t\ttemp = temp + bigram\n",
    "\t\t\t\t#row = XL[3].strip(' \\t\\n\\r')\n",
    "\t\t\t\t#temp = row[3].split()\n",
    "\t\t\t\t#N = numeral(temp)\n",
    "\t\t\t\tE = exclamation(row[3])\n",
    "\t\t\t\tQ = question(row[3])\n",
    "\t\t\t\tM = modal(temp)\n",
    "\t\t\t\tI = intensifier(temp)\n",
    "\t\t\t\tW = whword(temp)\n",
    "\t\t\t\tEP = event_phrase(temp)\n",
    "\t\t\t\tS = subjectivity(temp)\n",
    "\t\t\t\tSG = slang(temp)\n",
    "\t\t\t\tRL = religion(temp)\n",
    "\t\t\t\tP = pronoun(temp)\n",
    "\t\t\t\t#EM = emoticons(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t\t#SM = smileys(org_tweet[row[1].strip(' \\t\\n\\r')])\n",
    "\t\t\t\t#t = [N,E,Q,M,I,W,S,P,EP,SG,EM,SM]\n",
    "\t\t\t\tt = [N,E,Q,M,I,W,S,P,EP,SG,RL]\n",
    "\t\t\t\t#t = [N,E,Q,M,I,W,S,P,SG,RL]\n",
    "\t\t\t\tharda_feature.append(t)\n",
    "\t\t\t\tharda_label.append(int(row[4]))\n",
    "\t\t\t\tcount+=1\n",
    "\t\t\t\t'''if count==51:\n",
    "\t\t\t\t\tprint(row[1].strip(' \\t\\n\\r'))\n",
    "\t\t\t\t\tprint(t)\n",
    "\t\t\t\t\tsys.exit(0)'''\n",
    "\t\t\t\ttemp = []\n",
    "\t\t\t\tN = 0\n",
    "\n",
    "\tfp.close()\n",
    "\tfs.close()\n",
    "\n",
    "\tharda_clf = svm.SVC()\n",
    "\t#harda_clf = svm.SVC(kernel='rbf',gamma=0.5)\n",
    "\t#harda_clf = svm.SVC(kernel='linear')\n",
    "\t#harda_clf = svm.LinearSVC()\n",
    "\t#harda_clf = LogisticRegression()\n",
    "\t#harda_clf = BernoulliNB()\n",
    "\t#harda_clf = RandomForestClassifier()\n",
    "\tharda_clf.fit(harda_feature,harda_label)\n",
    "\tscores = cross_validation.cross_val_score(harda_clf,harda_feature,harda_label,cv=10)\n",
    "\tprint(scores.mean(),scores.std())\n",
    "\t#clf.cross_validation(feature,label)\n",
    "\tIN.append(scores.mean())\n",
    "\n",
    "\tprint('Nepal - Harda: ', nepal_clf.score(harda_feature,harda_label))\n",
    "\tprint('Harda - Nepal: ', harda_clf.score(nepal_feature,nepal_label))\n",
    "\tCR.append(nepal_clf.score(harda_feature,harda_label))\n",
    "\tCR.append(harda_clf.score(nepal_feature,nepal_label))\n",
    "\t\n",
    "\tharda_predicted_label = nepal_clf.predict(harda_feature)\n",
    "        print('Nepal -> Harda')\n",
    "        print('PRECISION: ',metrics.precision_score(harda_label,harda_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(harda_label,harda_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(harda_label,harda_predicted_label))\n",
    "\tPR.append(metrics.precision_score(harda_label,harda_predicted_label))\n",
    "\tRC.append(metrics.recall_score(harda_label,harda_predicted_label))\n",
    "\tFS.append(metrics.f1_score(harda_label,harda_predicted_label))\n",
    "\n",
    "\n",
    "\tnepal_predicted_label = harda_clf.predict(nepal_feature)\n",
    "        print('Harda -> Nepal')\n",
    "        print('PRECISION: ',metrics.precision_score(nepal_label,nepal_predicted_label))\n",
    "        print('RECALL: ',metrics.recall_score(nepal_label,nepal_predicted_label))\n",
    "        print('F1_SCORE: ',metrics.f1_score(nepal_label,nepal_predicted_label))\n",
    "\tPR.append(metrics.precision_score(nepal_label,nepal_predicted_label))\n",
    "\tRC.append(metrics.recall_score(nepal_label,nepal_predicted_label))\n",
    "\tFS.append(metrics.f1_score(nepal_label,nepal_predicted_label))\n",
    "\n",
    "\tprint(len(IN),len(CR),len(PR),len(RC),len(FS))\n",
    "        print('Average Indomain Accuracy: ',np.mean(IN))\n",
    "        print('Average Cross-domain Accuracy: ',np.mean(CR))\n",
    "        print('Average and std Precision: ',np.mean(PR),np.std(PR))\n",
    "        print('Average and std Recall: ',np.mean(RC),np.std(RC))\n",
    "        print('Average and std F-score: ',np.mean(FS),np.std(FS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "fs = codecs.open('/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/hindi/classification/dataset/harda_devanagari_train.txt','r','utf-8')\n",
    "fp = codecs.open('/home/sreenu/Desktop/rudra/koustav_phdthesis_2018-master/Chapter_3/hindi/classification/dataset/nepal_devanagari_train.txt','r','utf-8')\n",
    "for i in fp:\n",
    "    j=i.split('\\t')\n",
    "    print j[4]\n",
    "fp.close()\n",
    "fs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
